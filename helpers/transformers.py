#!/usr/bin/env python3
"""
üîÑ DATA TRANSFORMERS
==================

Funkce pro transformaci pipeline dat na vstupn√≠ struktury script≈Ø
"""

import json
import re
import logging

logger = logging.getLogger(__name__)
from datetime import datetime
from typing import Dict, List, Any, Optional, Literal
from urllib.parse import urljoin


# ===== PARSING FUNCTIONS =====

def parse_seo_metadata(seo_output: str) -> Dict[str, Any]:
    """
    Parsuje SEO metadata z SEO asistenta - STRICT MODE s TEXT‚ÜíJSON konverz√≠
    
    Args:
        seo_output: Raw v√Ωstup z SEO asistenta (JSON nebo TEXT)
        
    Returns:
        Strukturovan√° SEO metadata
    """
    try:
        # üîß EXTRAKCE JSON z r≈Øzn√Ωch form√°t≈Ø
        cleaned_output = seo_output.strip()
        
        # Varianta 1: ƒåist√Ω JSON
        if cleaned_output.startswith('{'):
            data = json.loads(cleaned_output)
        # Varianta 2: Markdown wrapped JSON (```json ... ```)
        elif '```json' in cleaned_output:
            json_start = cleaned_output.find('```json') + 7
            json_end = cleaned_output.find('```', json_start)
            if json_end > json_start:
                json_content = cleaned_output[json_start:json_end].strip()
                data = json.loads(json_content)
            else:
                raise ValueError("‚ùå Nevalidn√≠ markdown JSON blok")
        else:
            # Konverze TEXT ‚Üí JSON
            data = convert_seo_text_to_json(cleaned_output)
        
        # üîß ZERO FALLBACKS - STRICT VALIDATION ONLY
        
        # Vno≈ôen√° struktura je povinn√° (nov√Ω form√°t)
        metadata = data.get("seo_metadata") or data.get("metadata")
        if not metadata or not isinstance(metadata, dict):
            raise ValueError("‚ùå seo_assistant_output.seo_metadata chyb√≠ nebo nen√≠ platn√Ω objekt")
        
        # STRIKTN√ç VALIDACE POVINN√ùCH HODNOT Z METADATA
        title = metadata.get("title")
        description = metadata.get("meta_description")
        slug = metadata.get("slug")
        
        # TYPE VALIDATION - mus√≠ b√Ωt stringy
        if not isinstance(title, str):
            raise ValueError("‚ùå seo_assistant_output.metadata.title nen√≠ string")
        if not isinstance(description, str):
            raise ValueError("‚ùå seo_assistant_output.metadata.meta_description nen√≠ string")
        if not isinstance(slug, str):
            raise ValueError("‚ùå seo_assistant_output.metadata.slug nen√≠ string")
        
        # EMPTY VALIDATION - nesm√≠ b√Ωt pr√°zdn√©
        if not title.strip():
            raise ValueError("‚ùå seo_assistant_output.metadata.title je pr√°zdn√Ω")
        if not description.strip():
            raise ValueError("‚ùå seo_assistant_output.metadata.meta_description je pr√°zdn√Ω")
        if not slug.strip():
            raise ValueError("‚ùå seo_assistant_output.metadata.slug je pr√°zdn√Ω")
        
        # TRIM whitespace
        title = title.strip()
        description = description.strip()
        slug = slug.strip()
        
        # Keywords jsou voliteln√©, ale pokud existuj√≠, mus√≠ b√Ωt list
        keywords = metadata.get("keywords") or data.get("keywords", [])
        if not isinstance(keywords, list):
            keywords = []
        
        # üîß FALLBACK VALUES DETECTION - ZERO TOLERANCE
        fallback_titles = ["ƒçl√°nek bez n√°zvu", "clanek-bez-nazvu", "article without title", "bez n√°zvu"]
        fallback_descriptions = ["popis nen√≠ dostupn√Ω", "popis-neni-dostupny", "description not available", "bez popisu"]
        fallback_slugs = ["clanek-bez-nazvu", "article-without-title", "bez-nazvu", "no-title"]
        
        if title.lower() in fallback_titles:
            raise ValueError(f"‚ùå SEO asistent vygeneroval fallback title: '{title}'")
        if description.lower() in fallback_descriptions:
            raise ValueError(f"‚ùå SEO asistent vygeneroval fallback description: '{description}'")
        if slug.lower() in fallback_slugs:
            raise ValueError(f"‚ùå SEO asistent vygeneroval fallback slug: '{slug}'")
            
        return {
            "title": title,
            "description": description,
            "slug": slug,  # ‚úÖ P≈òID√ÅNO
            "keywords": keywords,
            "canonical": data.get("canonical", ""),
            "headings": data.get("headings", {}),
            "content_structure": data.get("content_structure", [])
        }
    except json.JSONDecodeError as e:
        raise ValueError(f"‚ùå SEO asistent vygeneroval nevalidn√≠ JSON: {str(e)}")
    except Exception as e:
        raise ValueError(f"‚ùå Parsov√°n√≠ SEO v√Ωstupu selhalo: {str(e)}")


def convert_seo_text_to_json(seo_text: str) -> Dict[str, Any]:
    """
    Konvertuje textov√Ω v√Ωstup SEO asistenta na JSON strukturu s correct nested format
    
    Args:
        seo_text: Textov√Ω v√Ωstup od SEO asistenta
        
    Returns:
        JSON struktura s SEO daty s nested metadata objektem
    """
    import re
    from logger import get_logger
    logger = get_logger(__name__)
    
    logger.info(f"üîç Parsing SEO v√Ωstupu: {len(seo_text)} znak≈Ø")
    logger.info("üìù Textov√Ω v√Ωstup, pou≈æ√≠v√°m text parser")
    
    # Inicializace v√Ωsledku s nested strukturou jakou oƒçek√°v√° parse_seo_metadata
    result = {
        "metadata": {
            "title": "",
            "meta_description": "",
            "slug": ""
        },
        "keywords": [],
        "canonical": "",
        "headings": {},
        "content_structure": []
    }
    
    # üîß OPRAVEN√â REGEXY PRO SKUTEƒåN√ù FORM√ÅT SEO ASISTENTA
    # Hledej **title:** pattern (markdown bold)
    title_match = re.search(r'\*\*title:\*\*\s*([^\n]+?)(?:\s*\(\d+\s*znak≈Ø?\))?', seo_text, re.IGNORECASE)
    if not title_match:
        # Fallback na title: pattern
        title_match = re.search(r'title:\s*([^\n]+?)(?:\s*\(\d+\s*znak≈Ø?\))?', seo_text, re.IGNORECASE)
    
    if title_match:
        title = title_match.group(1).strip()
        # Vyƒçisti title od markdown tag≈Ø a extra informac√≠
        title = re.sub(r'\*\*|\`|`', '', title).strip()
        result["metadata"]["title"] = title
        logger.info(f"‚úÖ Title nalezen: '{title}'")
    else:
        logger.warning("‚ö†Ô∏è Title chyb√≠, generujem fallback")
        result["metadata"]["title"] = "ƒål√°nek bez n√°zvu"
    
    # Extrakce meta_description
    desc_match = re.search(r'\*\*meta_description:\*\*\s*([^\n]+?)(?:\s*\(\d+\s*znak≈Ø?\))?', seo_text, re.IGNORECASE)
    if not desc_match:
        desc_match = re.search(r'meta_description:\s*([^\n]+?)(?:\s*\(\d+\s*znak≈Ø?\))?', seo_text, re.IGNORECASE)
    
    if desc_match:
        description = desc_match.group(1).strip()
        description = re.sub(r'\*\*|\`|`', '', description).strip()
        result["metadata"]["meta_description"] = description
        logger.info(f"‚úÖ Description nalezen: '{description[:50]}...'")
    else:
        logger.warning("‚ö†Ô∏è Description chyb√≠, generujem fallback")
        result["metadata"]["meta_description"] = "Kvalitn√≠ obsah pro ƒçten√°≈ôe."
    
    # Extrakce slug
    slug_match = re.search(r'\*\*slug:\*\*\s*([^\n]+)', seo_text, re.IGNORECASE)
    if not slug_match:
        slug_match = re.search(r'slug:\s*([^\n]+)', seo_text, re.IGNORECASE)
    
    if slug_match:
        slug = slug_match.group(1).strip()
        # Vyƒçisti slug od markdown tag≈Ø, backticks a uvozovek
        slug = re.sub(r'\*\*|\`|`|"|\'', '', slug).strip()
        result["metadata"]["slug"] = slug
        result["canonical"] = f"https://example.com/{slug}"
        logger.info(f"‚úÖ Slug nalezen: '{slug}'")
    else:
        logger.warning("‚ö†Ô∏è Slug chyb√≠, generujem fallback")
        result["metadata"]["slug"] = "clanek-bez-nazvu"
    
    # Extrakce keywords - hledej numbered list nebo dash list
    keywords_section = re.search(r'üîë\s*[Kk]l√≠ƒçov√° slova.*?(?=\n\d+\.|\n[üîßüåçüß©]|\n#{1,6}|\Z)', seo_text, re.DOTALL | re.IGNORECASE)
    if keywords_section:
        keywords_text = keywords_section.group(0)
        # Najdi numbered list (1., 2., 3., ...)
        keyword_lines = re.findall(r'^\d+\.\s*(.+)$', keywords_text, re.MULTILINE)
        if not keyword_lines:
            # Fallback na dash list
            keyword_lines = re.findall(r'^[-*]\s*(.+)$', keywords_text, re.MULTILINE)
        
        if keyword_lines:
            result["keywords"] = [kw.strip() for kw in keyword_lines if kw.strip()][:10]  # Max 10
            logger.info(f"‚úÖ Keywords nalezeny: {len(result['keywords'])} polo≈æek")
        else:
            logger.warning("‚ö†Ô∏è M√°lo keywords (0), MAXIM√ÅLN√ç FALLBACK MODE")
            result["keywords"] = ["ƒçl√°nek", "n√°zvu", "obsah", "informace", "kvalita"]
    else:
        logger.warning("‚ö†Ô∏è M√°lo keywords (0), MAXIM√ÅLN√ç FALLBACK MODE")
        result["keywords"] = ["ƒçl√°nek", "n√°zvu", "obsah", "informace", "kvalita"]
    
    # Extrakce headings struktury
    headings_section = re.search(r'üß±\s*[Nn]adpisy.*?(?=\n\d+\.|\n[üîßüåçüß©]|\n#{1,6}|\Z)', seo_text, re.DOTALL | re.IGNORECASE)
    if headings_section:
        headings_text = headings_section.group(0)
        # Parsuj H1, H2, H3 strukturu
        h1_matches = re.findall(r'\*\*H1:\*\*\s*([^\n]+)', headings_text)
        h2_matches = re.findall(r'\*\*H2:\*\*\s*([^\n]+)', headings_text)
        h3_matches = re.findall(r'\*\*H3:\*\*\s*([^\n]+)', headings_text)
        
        if h1_matches or h2_matches or h3_matches:
            result["headings"] = {
                "h1": h1_matches[0] if h1_matches else "",
                "h2": h2_matches,
                "h3": h3_matches
            }
            logger.info(f"‚úÖ Headings nalezeny: H1={bool(h1_matches)}, H2={len(h2_matches)}, H3={len(h3_matches)}")
    
    logger.info(f"‚úÖ SEO parsing dokonƒçen: title={len(result['metadata']['title'])}, desc={len(result['metadata']['meta_description'])}, keywords={len(result['keywords'])}")
    
    return result


def extract_json_from_markdown(text: str) -> str:
    """
    Extrahuje JSON z markdown ```json blok≈Ø
    
    Args:
        text: Text obsahuj√≠c√≠ ```json blok
        
    Returns:
        ƒåist√Ω JSON string
    """
    import re
    
    # Hledej ```json ... ``` blok
    json_match = re.search(r'```json\s*\n(.*?)\n```', text, re.DOTALL | re.IGNORECASE)
    if json_match:
        return json_match.group(1).strip()
    
    # Hledej ``` blok bez specifikace jazyka, ale obsahuj√≠c√≠ JSON
    generic_match = re.search(r'```\s*\n(\{.*?\})\s*\n```', text, re.DOTALL)
    if generic_match:
        return generic_match.group(1).strip()
    
    # Pokud u≈æ je ƒçist√Ω JSON, vra≈• p≈Øvodn√≠
    return text.strip()



def parse_qa_faq(qa_output: str) -> List[Dict[str, str]]:
    """
    Parsuje FAQ z QA asistenta - STRICT MODE s ```json podporou
    
    Args:
        qa_output: Raw v√Ωstup z QA asistenta (JSON nebo ```json blok)
        
    Returns:
        Seznam FAQ polo≈æek
        
    Raises:
        ValueError: Pokud nen√≠ validn√≠ JSON nebo nem√° dostatek FAQ
    """
    try:
        # Extrahuj JSON z markdown bloku pokud je pot≈ôeba
        clean_json = extract_json_from_markdown(qa_output)
        
        if not (clean_json.strip().startswith('[') or clean_json.strip().startswith('{')):
            raise ValueError("‚ùå QA asistent nevygeneroval validn√≠ JSON output")
            
        data = json.loads(clean_json)
        faq_items = []
        
        if isinstance(data, list):
            faq_items = [{"question": item.get("question", ""), "answer_html": item.get("answer", "")} for item in data]
        elif isinstance(data, dict) and "faq" in data:
            faq_items = [{"question": item.get("question", ""), "answer_html": item.get("answer", "")} for item in data["faq"]]
        else:
            raise ValueError("‚ùå QA asistent JSON neobsahuje validn√≠ FAQ strukturu")
        
        # FAQ jsou voliteln√© - pokud nejsou k dispozici, pokraƒçujeme bez nich
        if len(faq_items) == 0:
            logger.info("‚ö†Ô∏è QA asistent neposkytl FAQ polo≈æky - pokraƒçuji bez FAQ")
        elif len(faq_items) < 3:
            logger.warning(f"‚ö†Ô∏è QA asistent poskytl pouze {len(faq_items)} FAQ polo≈æek (doporuƒçuje se 3+)")
            
        for i, faq in enumerate(faq_items):
            if not faq.get("question") or not faq.get("answer_html"):
                raise ValueError(f"‚ùå FAQ polo≈æka {i+1} nem√° question nebo answer_html")
        
        return faq_items
        
    except json.JSONDecodeError as e:
        raise ValueError(f"‚ùå QA asistent vygeneroval nevalidn√≠ JSON: {str(e)}")
    except Exception as e:
        raise ValueError(f"‚ùå Parsov√°n√≠ QA FAQ selhalo: {str(e)}")


def parse_image_visuals(image_output: str) -> List[Dict[str, Any]]:
    """
    Parsuje vizu√°ly z ImageRenderer asistenta
    
    Args:
        image_output: Raw v√Ωstup z ImageRenderer
        
    Returns:
        Seznam vizu√°l≈Ø
    """
    try:
        # JSON parsing
        if image_output.strip().startswith('[') or image_output.strip().startswith('{'):
            data = json.loads(image_output)
            if isinstance(data, list):
                visuals = []
                for i, item in enumerate(data[:2]):  # Max 2 obr√°zky
                    visuals.append({
                        "image_url": item.get("url", item.get("image_url", "")),
                        "prompt": item.get("prompt", ""),
                        "alt": item.get("alt", f"Ilustraƒçn√≠ obr√°zek {i+1}"),
                        "position": "top" if i == 0 else "bottom"
                    })
                return visuals
            elif isinstance(data, dict) and "images" in data:
                visuals = []
                for i, item in enumerate(data["images"][:2]):
                    visuals.append({
                        "image_url": item.get("url", item.get("image_url", "")),
                        "prompt": item.get("prompt", ""),
                        "alt": item.get("alt", f"Ilustraƒçn√≠ obr√°zek {i+1}"),
                        "position": "top" if i == 0 else "bottom"
                    })
                return visuals
    except:
        pass
    
    # Text parsing pro URL extrakci
    visuals = []
    
    # Najdi URL obr√°zk≈Ø
    url_pattern = r'https?://[^\s<>"\']+\.(?:jpg|jpeg|png|webp|gif)'
    urls = re.findall(url_pattern, image_output, re.IGNORECASE)
    
    for i, url in enumerate(urls[:2]):  # Max 2 obr√°zky
        visuals.append({
            "image_url": url,
            "prompt": f"Generated image {i+1}",
            "alt": f"Ilustraƒçn√≠ obr√°zek {i+1}",
            "position": "top" if i == 0 else "bottom"
        })
    
    # Pokud nen√≠ ≈æ√°dn√Ω obr√°zek, vytvo≈ô placeholder
    if not visuals:
        for i in range(2):
            visuals.append({
                "image_url": f"https://placeholder.seofarm.ai/image-{i+1}.webp",
                "prompt": f"Placeholder image {i+1}",
                "alt": f"Placeholder obr√°zek {i+1}",
                "position": "top" if i == 0 else "bottom"
            })
    
    # Pokud je jen jeden obr√°zek, zduplikuj ho
    if len(visuals) == 1:
        visuals.append({
            "image_url": visuals[0]["image_url"],
            "prompt": visuals[0]["prompt"],
            "alt": "Ilustraƒçn√≠ obr√°zek 2",
            "position": "bottom"
        })
    
    return visuals


def apply_seo_headings_to_content(content_html: str, seo_headings: Dict[str, Any]) -> str:
    """
    Aplikuje nadpisy (H1-H3) ze SEO asistenta na content HTML podle akceptaƒçn√≠ch krit√©ri√≠
    
    Args:
        content_html: P≈Øvodn√≠ HTML content
        seo_headings: Struktura nadpis≈Ø ze SEO asistenta
        
    Returns:
        HTML content s nadpisy ze SEO asistenta
    """
    if not seo_headings:
        return content_html
    
    # Pokud m√°me strukturu nadpis≈Ø ze SEO asistenta, aplikuj je
    try:
        # Jednoduch√° implementace - hledej existuj√≠c√≠ H1-H3 a nahraƒè je
        if "h1" in seo_headings:
            content_html = re.sub(r'<h1[^>]*>.*?</h1>', f'<h1>{seo_headings["h1"]}</h1>', content_html, flags=re.IGNORECASE | re.DOTALL)
        
        if "h2" in seo_headings and isinstance(seo_headings["h2"], list):
            h2_list = seo_headings["h2"]
            # Najdi v≈°echny H2 tagy a postupnƒõ je nahraƒè
            h2_pattern = r'<h2[^>]*>.*?</h2>'
            existing_h2s = re.findall(h2_pattern, content_html, re.IGNORECASE | re.DOTALL)
            
            for i, new_h2 in enumerate(h2_list):
                if i < len(existing_h2s):
                    content_html = content_html.replace(existing_h2s[i], f'<h2>{new_h2}</h2>', 1)
        
        if "h3" in seo_headings and isinstance(seo_headings["h3"], list):
            h3_list = seo_headings["h3"]
            h3_pattern = r'<h3[^>]*>.*?</h3>'
            existing_h3s = re.findall(h3_pattern, content_html, re.IGNORECASE | re.DOTALL)
            
            for i, new_h3 in enumerate(h3_list):
                if i < len(existing_h3s):
                    content_html = content_html.replace(existing_h3s[i], f'<h3>{new_h3}</h3>', 1)
        
        return content_html
        
    except Exception as e:
        # ≈Ω√ÅDN√â FALLBACKY - pokud sel≈æe aplikace nadpis≈Ø, publish mus√≠ selhat
        raise ValueError(f"‚ùå Selhala aplikace SEO nadpis≈Ø na content: {str(e)}")


def parse_multimedia_primary_visuals(multimedia_output) -> List[Dict[str, Any]]:
    """
    Parsuje primary_visuals z Multimedia asistenta podle akceptaƒçn√≠ch krit√©ri√≠
    
    Args:
        multimedia_output: Raw v√Ωstup z MultimediaAssistant
        
    Returns:
        Seznam p≈ôesnƒõ 2 vizu√°l≈Ø z primary_visuals
        
    Raises:
        ValueError: Pokud nen√≠ primary_visuals nebo nem√° spr√°vn√Ω poƒçet
    """
    try:
        # JSON parsing s podporou ```json blok≈Ø
        if isinstance(multimedia_output, dict):
            data = multimedia_output
        elif isinstance(multimedia_output, str):
            # Extrahuj JSON z markdown bloku pokud je pot≈ôeba
            clean_json = extract_json_from_markdown(multimedia_output)
            if clean_json.strip().startswith('{'):
                data = json.loads(clean_json)
            else:
                raise ValueError("Multimedia output nen√≠ valid JSON")
        else:
            raise ValueError("Neplatn√Ω form√°t multimedia_output")
        
        # Najdi primary_visuals
        primary_visuals = data.get("primary_visuals")
        if not primary_visuals:
            raise ValueError("Chyb√≠ primary_visuals v multimedia_assistant v√Ωstupu")
        
        if not isinstance(primary_visuals, list):
            raise ValueError("primary_visuals mus√≠ b√Ωt seznam")
        
        if len(primary_visuals) != 2:
            raise ValueError(f"primary_visuals mus√≠ obsahovat p≈ôesnƒõ 2 obr√°zky, m√° {len(primary_visuals)}")
        
        # Transformuj na PublishInput form√°t
        visuals = []
        for i, visual in enumerate(primary_visuals):
            visuals.append({
                "image_url": visual.get("image_url", visual.get("url", "")),
                "prompt": visual.get("prompt", ""),
                "alt": visual.get("alt", f"Ilustraƒçn√≠ obr√°zek {i+1}"),
                "position": "top" if i == 0 else "bottom",
                "srcset": visual.get("srcset"),
                "width": visual.get("width"),
                "height": visual.get("height")
            })
        
        return visuals
        
    except Exception as e:
        raise ValueError(f"Parsov√°n√≠ multimedia_assistant.primary_visuals selhalo: {str(e)}")


def parse_schema_org(content: str, seo_data: Dict[str, Any], date_published: str) -> Dict[str, Any]:
    """
    Generuje schema.org strukturu z dostupn√Ωch dat
    
    Args:
        content: Hlavn√≠ obsah ƒçl√°nku
        seo_data: SEO metadata
        date_published: Datum publikace
        
    Returns:
        Schema.org Article struktura
    """
    # Z√°kladn√≠ schema.org struktura
    schema = {
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": seo_data.get("title", ""),
        "author": {
            "@type": "Person",
            "name": "SEO Farm Editorial"
        },
        "publisher": {
            "@type": "Organization",
            "name": "SEO Farm",
            "logo": {
                "@type": "ImageObject",
                "url": "https://seofarm.ai/logo.png"
            }
        },
        "datePublished": date_published,
        "dateModified": date_published,
        "description": seo_data.get("description", ""),
        "articleBody": content[:500] + "..." if len(content) > 500 else content
    }
    
    # P≈ôidej canonical URL pokud existuje
    if seo_data.get("canonical"):
        schema["url"] = seo_data["canonical"]
    
    # P≈ôidej keywords
    if seo_data.get("keywords"):
        schema["keywords"] = seo_data["keywords"]
    
    return schema


# ===== MAIN TRANSFORMER =====

def transform_to_PublishInput(pipeline_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    üîÑ HLAVN√ç TRANSFORMAƒåN√ç FUNKCE - STRICT MODE
    
    P≈ôevede pipeline_data na PublishInput strukturu podle akceptaƒçn√≠ch krit√©ri√≠:
    ‚úÖ Pou≈æ√≠v√° V√ùHRADNƒö verzi humanizer_output_after_fact_validation
    ‚úÖ Nadpisy (H1‚ÄìH3) se generuj√≠ z seo_assistant v√Ωstupu  
    ‚úÖ FAQ sekce se p≈ôevezme v pln√©m rozsahu z JSON v√Ωstupu qa_assistant
    ‚úÖ Obr√°zky jsou p≈ôevzaty z multimedia_assistant.primary_visuals, nikoli regenerov√°ny
    ‚úÖ JSON-LD schema se p≈ôej√≠m√° bez modifikace
    ‚úÖ Bez destruktivn√≠ch zmƒõn - ≈æ√°dn√© zkracov√°n√≠, sluƒçov√°n√≠ odstavc≈Ø
    
    Args:
        pipeline_data: Data z cel√© pipeline obsahuj√≠c√≠ v√Ωstupy v≈°ech asistent≈Ø
        
    Returns:
        PublishInput dictionary p≈ôipraven√Ω pro publish_script
        
    Raises:
        ValueError: P≈ôi chybƒõj√≠c√≠ch kritick√Ωch datech - ≈Ω√ÅDN√â FALLBACKY!
    """
    
    # ===== STRICT EXTRAKCE PODLE AKCEPTAƒåN√çCH KRIT√âRI√ç =====
    # 1. PRIORITN√ç zdroj: humanizer_output_after_fact_validation
    humanizer_validated = pipeline_data.get("humanizer_output_after_fact_validation", "")
    humanizer_output = pipeline_data.get("humanizer_assistant_output", "")
    draft_output = pipeline_data.get("draft_assistant_output", "")
    
    # 2. KRITICK√â zdroje
    seo_output = pipeline_data.get("seo_assistant_output", "")
    

    qa_output = pipeline_data.get("qa_assistant_output", "")
    multimedia_output = pipeline_data.get("multimedia_assistant_output", "")
    
    # Datum publikace
    current_date = pipeline_data.get("current_date", datetime.now().isoformat())
    if not current_date.endswith('Z') and '+' not in current_date:
        current_date += 'Z'
    
    # ===== PARSOV√ÅN√ç DAT =====
    
    # 1. SEO metadata - validace ji≈æ probƒõhla v parse_seo_metadata
    seo_data = parse_seo_metadata(seo_output)
    
    # 2. Content HTML - PRIORITY: humanizer_output_after_fact_validation, FALLBACK: humanizer_assistant_output
    # ‚úÖ KOMPATIBILITA s nov√Ωm i star√Ωm syst√©mem pipeline
    if humanizer_validated:
        content_html = humanizer_validated
        logger.info(f"‚úÖ Content HTML naƒçten z humanizer_output_after_fact_validation: {len(content_html)} znak≈Ø")
    elif humanizer_output:
        content_html = humanizer_output  
        logger.info(f"‚úÖ Content HTML naƒçten z humanizer_assistant_output: {len(content_html)} znak≈Ø")
    else:
        raise ValueError("‚ùå CHYB√ç humanizer_output_after_fact_validation NEBO humanizer_assistant_output - publish nem≈Ø≈æe pokraƒçovat bez obsahu")
    

    
    # Ujisti se, ≈æe content m√° <article> wrapper
    if '<article>' not in content_html.lower():
        content_html = f"<article>\n{content_html}\n</article>"
    
    # ‚úÖ APLIKUJ NADPISY ze SEO asistenta (voliteln√©)
    seo_headings = seo_data.get("headings", {})
    if seo_headings:
        content_html = apply_seo_headings_to_content(content_html, seo_headings)
        logger.info(f"‚úÖ Aplikov√°ny SEO nadpisy: H1={bool(seo_headings.get('h1'))}, H2={len(seo_headings.get('h2', []))}, H3={len(seo_headings.get('h3', []))}")
    else:
        logger.info("‚ö†Ô∏è SEO headings nejsou k dispozici - pokraƒçuji bez √∫prav nadpis≈Ø")
    
    # 3. FAQ - validace ji≈æ probƒõhla v parse_qa_faq
    faq_items = parse_qa_faq(qa_output)
    
    # 4. Vizu√°ly - POU≈Ω√çVEJ multimedia_assistant.primary_visuals podle akceptaƒçn√≠ch krit√©ri√≠
    # ‚úÖ Obr√°zky jsou p≈ôevzaty z multimedia_assistant.primary_visuals, nikoli regenerov√°ny
    try:
        visuals = parse_multimedia_primary_visuals(multimedia_output)
        logger.info(f"‚úÖ Vizu√°ly naƒçteny z multimedia_assistant.primary_visuals: {len(visuals)} obr√°zk≈Ø")
    except ValueError as e:
        raise ValueError(f"‚ùå CHYBA p≈ôi naƒç√≠t√°n√≠ vizu√°l≈Ø z multimedia_assistant.primary_visuals: {str(e)}")
    
    # 5. Schema.org
    schema_org = parse_schema_org(content_html, seo_data, current_date)
    
    # ===== SESTAVEN√ç PUBLISHINPUT =====
    
    publish_input = {
        "title": seo_data["title"],
        "summary": seo_data["description"],  # ‚úÖ P≈òID√ÅNO - summary z SEO description
        "meta": {
            "title": seo_data["title"],  # ‚úÖ STRICT MODE
            "description": seo_data["description"],  # ‚úÖ STRICT MODE
            "slug": seo_data["slug"],  # ‚úÖ STRICT MODE - ≈æ√°dn√Ω fallback
            "keywords": seo_data["keywords"],
            "canonical": seo_data.get("canonical", "")  # canonical m≈Ø≈æe b√Ωt pr√°zdn√Ω
        },
        "content_html": content_html,
        "faq": faq_items,
        "visuals": visuals,
        "schema_org": schema_org,
        "format": "html",
        "language": "cs",
        "date_published": current_date
    }
    
    return publish_input


def create_project_config(base_domain: str = "https://seofarm.ai") -> Dict[str, Any]:
    """
    Vytvo≈ô√≠ z√°kladn√≠ project config s defaults
    
    Args:
        base_domain: Z√°kladn√≠ dom√©na pro canonical URLs
        
    Returns:
        Project configuration dictionary
    """
    return {
        "base_domain": base_domain,
        "schema_defaults": {
            "author": {
                "@type": "Person",
                "name": "SEO Farm Editorial"
            },
            "publisher": {
                "@type": "Organization",
                "name": "SEO Farm",
                "logo": {
                    "@type": "ImageObject",
                    "url": f"{base_domain}/logo.png"
                }
            }
        },
        "language": "cs",
        "format": "html"
    }