"""
üõ°Ô∏è BEZPEƒåN√â ASSISTANT AKTIVITY PRO PRODUKƒåN√ç PROST≈òED√ç
Stabiln√≠ verze assistant aktivit s kompletn√≠m error handlingem.
"""

import json
import asyncio
from typing import Dict, Any, Optional, List
from datetime import datetime
from temporalio import activity

# Na≈°e nov√© bezpeƒçn√© moduly
from activity_wrappers import safe_activity, validate_activity_input, safe_llm_call
from logger import get_logger, log_llm_request, log_llm_response
from config import get_llm_config, get_activity_config

# Origin√°ln√≠ z√°vislosti
from backend.llm_clients.factory import LLMClientFactory
from prisma import Prisma

logger = get_logger(__name__)

@safe_activity(name="load_assistants_from_database", timeout_seconds=60)
async def load_assistants_from_database(project_id: str) -> Dict[str, Any]:
    """
    Bezpeƒçnƒõ naƒçte asistenty z datab√°ze pro dan√Ω projekt.
    
    Args:
        project_id: ID projektu
        
    Returns:
        Dict s naƒçten√Ωmi asistenty
    """
    validate_activity_input({"project_id": project_id}, ["project_id"])
    
    if not project_id or not project_id.strip():
        raise ValueError("Project ID nesm√≠ b√Ωt pr√°zdn√©")
    
    db = None
    try:
        db = Prisma()
        await db.connect()
        
        # Naƒçten√≠ pouze aktivn√≠ch asistent≈Ø (bez order_by - se≈ôad√≠me v Pythonu)
        assistants = await db.assistant.find_many(
            where={"projectId": project_id, "active": True}
        )
        
        logger.info(f"üìä Naƒçteno {len(assistants)} asistent≈Ø z datab√°ze pro projekt {project_id}")
        
        # Se≈ôazen√≠ podle order pole v Pythonu
        assistants = sorted(assistants, key=lambda x: x.order if x.order is not None else 0)
        
        if not assistants:
            logger.warning(f"‚ö†Ô∏è ≈Ω√°dn√≠ asistenti nebyli nalezeni pro projekt {project_id}")
            return {
                "status": "warning",
                "assistants": [],
                "count": 0,
                "message": f"≈Ω√°dn√≠ asistenti pro projekt {project_id}"
            }
        
        # Validace asistent≈Ø
        valid_assistants = []
        for assistant in assistants:
            if not assistant.name or not assistant.name.strip():
                logger.error(f"‚ùå Asistent {assistant.id} m√° pr√°zdn√Ω n√°zev - p≈ôeskakuji")
                continue
            
            if not assistant.functionKey:
                logger.error(f"‚ùå Asistent {assistant.name} nem√° functionKey - p≈ôeskakuji")
                continue
                
            valid_assistants.append({
                "id": assistant.id,
                "name": assistant.name,
                "function_key": assistant.functionKey,
                "system_prompt": assistant.system_prompt or "",
                "model_provider": assistant.model_provider or "openai", 
                "model": assistant.model or "gpt-3.5-turbo",
                "temperature": assistant.temperature or 0.7,
                "max_tokens": assistant.max_tokens,
                "order": assistant.order or 0
            })
        
        logger.info(f"‚úÖ Naƒçteno {len(valid_assistants)} validn√≠ch asistent≈Ø z {len(assistants)} celkem")
        
        return {
            "status": "completed",
            "assistants": valid_assistants,
            "count": len(valid_assistants),
            "project_id": project_id
        }
        
    finally:
        if db:
            await db.disconnect()

@safe_activity(name="execute_assistant", timeout_seconds=600, heartbeat_interval=30)
async def execute_assistant(args: Dict[str, Any]) -> Dict[str, Any]:
    """
    Bezpeƒçnƒõ spust√≠ jednoho asistenta s LLM vol√°n√≠m.
    
    Args:
        args: Dictionary obsahuj√≠c√≠:
            - assistant_config: Konfigurace asistenta
            - topic: T√©ma/vstup pro asistenta
            - current_date: Aktu√°ln√≠ datum (optional)
            - previous_outputs: V√Ωstupy p≈ôedchoz√≠ch asistent≈Ø (optional)
        
    Returns:
        Dict s v√Ωstupem asistenta
    """
    # Extrakce argument≈Ø z args dictionary
    assistant_config = args.get("assistant_config")
    topic = args.get("topic")
    current_date = args.get("current_date")
    previous_outputs = args.get("previous_outputs", {})
    
    validate_activity_input(
        {"assistant_config": assistant_config, "topic": topic},
        ["assistant_config", "topic"]
    )
    
    assistant_name = assistant_config.get("name", "Unknown")
    function_key = assistant_config.get("function_key", "unknown")
    
    logger.info(f"üöÄ Spou≈°t√≠m asistenta: {assistant_name} ({function_key})")
    
    # Validace konfigurace
    required_fields = ["name", "function_key", "model_provider", "model"]
    missing_fields = [field for field in required_fields if not assistant_config.get(field)]
    if missing_fields:
        raise ValueError(f"Chyb√≠ povinn√° pole v assistant_config: {missing_fields}")
    
    # P≈ô√≠prava vstupn√≠ch dat
    if topic is None:
        raise ValueError("Topic nesm√≠ b√Ωt None")
    
    # üîß OPRAVA: Topic m≈Ø≈æe b√Ωt dict z p≈ôedchoz√≠ho asistenta - p≈ôev√°d√≠me na string
    if isinstance(topic, dict):
        # Pokud je dict, vezmi "output" kl√≠ƒç nebo cel√Ω JSON
        import json
        topic = topic.get("output", json.dumps(topic, ensure_ascii=False))
        logger.info(f"üîÑ Topic je dict - p≈ôev√°d√≠m na string ({len(str(topic))} chars)")
    elif not isinstance(topic, str):
        topic = str(topic)
        logger.info(f"üîÑ Topic nen√≠ string - p≈ôev√°d√≠m ({type(topic).__name__} -> str)")
    
    if not topic.strip():
        logger.warning(f"‚ö†Ô∏è Pr√°zdn√Ω topic pro {assistant_name} - pokraƒçuji")
        topic = ""
    
    # P≈ô√≠prava user message s datem
    user_message = topic
    if current_date:
        user_message = f"üìÖ Aktu√°ln√≠ datum: {current_date}\n\n{topic}"
    
    # Speci√°ln√≠ handling pro ImageRendererAssistant - pou≈æij data od MultimediaAssistant
    if function_key == "image_renderer_assistant" and previous_outputs:
        multimedia_output = previous_outputs.get("MultimediaAssistant", {}).get("output")
        if multimedia_output:
            try:
                # MultimediaAssistant nyn√≠ vrac√≠ nativn√≠ objekt, ne string
                if isinstance(multimedia_output, str):
                    # Backward compatibility - star≈°√≠ verze vracely string
                    import json
                    image_prompts = json.loads(multimedia_output)
                else:
                    # Nov√° verze - nativn√≠ objekt
                    image_prompts = multimedia_output
                
                if image_prompts and len(image_prompts) > 0:
                    # Pou≈æij prvn√≠ image prompt od MultimediaAssistant
                    first_prompt = image_prompts[0].get("image_prompt", user_message)
                    user_message = first_prompt
                    logger.info(f"üé® ImageRenderer pou≈æ√≠v√° prompt od MultimediaAssistant: {first_prompt[:100]}...")
                else:
                    # ‚ùå ≈Ω√ÅDN√â FALLBACKY - pipeline mus√≠ selhat
                    raise ValueError("MultimediaAssistant nevr√°til ≈æ√°dn√© validn√≠ image prompty")
            except (json.JSONDecodeError, KeyError, AttributeError) as e:
                # ‚ùå ≈Ω√ÅDN√â FALLBACKY - pipeline mus√≠ selhat
                raise ValueError(f"Chyba p≈ôi parsov√°n√≠ MultimediaAssistant output: {e}")
        else:
            # ‚ùå ≈Ω√ÅDN√â FALLBACKY - pipeline mus√≠ selhat
            raise ValueError("ImageRenderer nena≈°el v√Ωstup od MultimediaAssistant")
    
    # Z√≠sk√°n√≠ LLM konfigurace
    llm_config = get_llm_config()
    model_provider = assistant_config.get("model_provider", "openai")
    model = assistant_config.get("model", "gpt-3.5-turbo")
    temperature = assistant_config.get("temperature", llm_config.default_temperature)
    max_tokens = assistant_config.get("max_tokens", llm_config.default_max_tokens)
    system_prompt = assistant_config.get("system_prompt", "")
    
    # Speci√°ln√≠ handling pro -1 (unlimited)
    if max_tokens == -1:
        max_tokens = None
    
    logger.info(f"ü§ñ LLM konfigurace: {model_provider}/{model} (temp: {temperature}, tokens: {max_tokens or 'unlimited'})")
    
    # Vytvo≈ôen√≠ LLM clienta
    try:
        llm_client = LLMClientFactory.create_client(model_provider)
        log_llm_request(logger, model_provider, model, max_tokens)
        
        # Bezpeƒçn√© LLM vol√°n√≠ - rozpozn√°n√≠ typu modelu
        start_time = datetime.now()
        
        # Detekce image model≈Ø (DALL-E) - SPECI√ÅLN√ç HANDLING PRO IMAGERENDERER
        image_models = ["dall-e-3", "dall-e-2", "dall-e"]
        is_image_model = any(img_model in model.lower() for img_model in image_models)
        
        if is_image_model and function_key == "image_renderer_assistant":
            # üéØ SPECI√ÅLN√ç LOGIKA PRO IMAGERENDERER - VYGENERUJ V≈†ECHNY OBR√ÅZKY
            logger.info(f"üé® IMAGERENDERER SPECI√ÅLN√ç HANDLING: model={model}")
            
            try:
                # Extrahuj image_prompts z MultimediaAssistant v√Ωstupu
                image_prompts = await _extract_image_prompts_from_input(user_message)
                logger.info(f"üîç Nalezeno {len(image_prompts)} image_prompts")
                
                if not image_prompts:
                    logger.warning("‚ö†Ô∏è ImageRenderer: ≈Ω√°dn√© image_prompts nalezeny v inputu")
                    # Vra≈• pr√°zdn√Ω ale validn√≠ v√Ωsledek m√≠sto exception
                    final_result = {
                        "images": [],
                        "successful_count": 0,
                        "total_count": 0,
                        "model": model,
                        "config": {"provider": model_provider},
                        "warning": "No image_prompts found in input"
                    }
                    
                    return {
                        "status": "completed", 
                        "output": final_result,  # ‚Üê NATIVN√ç OBJEKT, NE STRING!
                        "assistant": assistant_name,
                        "function_key": function_key,
                        "provider": model_provider,
                        "model": model,
                        "duration": (datetime.now() - start_time).total_seconds(),
                        "input_length": len(user_message),
                        "output_length": len(str(final_result))
                    }
                
                logger.info(f"üé® Nalezeno {len(image_prompts)} image_prompts k vygenerov√°n√≠")
                
                # Vygeneruj obr√°zek pro ka≈æd√Ω prompt
                generated_images = []
                
                for i, prompt in enumerate(image_prompts):
                    logger.info(f"üé® Generuji obr√°zek {i+1}/{len(image_prompts)}: {prompt[:100]}...")
                    
                    # T≈ôi pokusy pro ka≈æd√Ω obr√°zek
                    llm_result = None
                    last_error = None
                    
                    for attempt in range(3):
                        try:
                            activity.heartbeat()
                            logger.info(f"ü§ñ IMAGE pokus {attempt + 1}/3 pro prompt {i+1}: {model_provider}/{model}")
                            
                            llm_result = await llm_client.image_generation(
                                prompt=prompt,
                                size="1024x1024",
                                quality="standard", 
                                style="vivid"
                            )
                            
                            if llm_result and "content" in llm_result:
                                logger.info(f"‚úÖ IMAGE √∫spƒõch pro prompt {i+1}: {model_provider}/{model}")
                                break
                            else:
                                raise Exception(f"Image generation vr√°til nevalidn√≠ response: {llm_result}")
                                
                        except Exception as e:
                            last_error = e
                            logger.warning(f"‚ö†Ô∏è IMAGE pokus {attempt + 1} pro prompt {i+1} selhal: {str(e)}")
                            if attempt < 2:
                                await asyncio.sleep(2 ** attempt)
                    
                    if not llm_result or "content" not in llm_result:
                        logger.error(f"‚ùå Image generation selhal pro prompt {i+1}: {last_error}")
                        # P≈ôidej chybn√Ω obr√°zek m√≠sto exception
                        generated_images.append({
                            "url": "",
                            "prompt": prompt,
                            "status": "failed",
                            "error": f"DALL-E generation failed: {last_error}"
                        })
                        continue
                    
                    # Extrahuj URL z response
                    image_url = await _extract_url_from_dalle_response(llm_result)
                    if not image_url:
                        logger.warning(f"‚ö†Ô∏è ≈Ω√°dn√° URL nalezena v DALL-E response pro prompt {i+1}")
                        generated_images.append({
                            "url": "",
                            "prompt": prompt,
                            "status": "failed",
                            "error": "No URL found in DALL-E response"
                        })
                        continue
                    
                    generated_images.append({
                        "url": image_url,
                        "prompt": prompt,
                        "status": "completed"
                    })
                    
                    logger.info(f"‚úÖ Obr√°zek {i+1} vygenerov√°n: {image_url[:100]}...")
                
                # Vytvo≈ô fin√°ln√≠ output jako NATIVN√ç OBJEKT (ne string!)
                # I kdy≈æ nƒõkter√© obr√°zky selhaly, vra≈• co se poda≈ôilo vygenerovat
                successful_images = [img for img in generated_images if img["status"] == "completed"]
                
                final_result = {
                    "images": generated_images,  # Zahrnuj v≈°echny obr√°zky (i chybn√©)
                    "successful_count": len(successful_images),
                    "total_count": len(image_prompts),
                    "model": model,
                    "config": {"provider": model_provider}
                }
                
                logger.info(f"üéâ ImageRenderer HOTOVO: {len(generated_images)} obr√°zk≈Ø vygenerov√°no!")
                
                # Vra≈• jako nativn√≠ objekt pro workflow
                return {
                    "status": "completed", 
                    "output": final_result,  # ‚Üê NATIVN√ç OBJEKT, NE STRING!
                    "assistant": assistant_name,
                    "function_key": function_key,
                    "provider": model_provider,
                    "model": model,
                    "duration": (datetime.now() - start_time).total_seconds(),
                    "input_length": len(user_message),
                    "output_length": len(str(final_result))
                }
            
            except Exception as e:
                logger.error(f"‚ùå CHYBA v ImageRenderer speci√°ln√≠ logice: {e}")
                # Vra≈• chybov√Ω v√Ωsledek jako nativn√≠ objekt - NIKDY nepokraƒçuj do elif is_image_model
                error_result = {
                    "images": [],
                    "successful_count": 0,
                    "total_count": 0,
                    "model": model,
                    "config": {"provider": model_provider},
                    "error": f"ImageRenderer logic failed: {str(e)}"
                }
                
                return {
                    "status": "completed", 
                    "output": error_result,  # ‚Üê NATIVN√ç OBJEKT i p≈ôi chybƒõ!
                    "assistant": assistant_name,
                    "function_key": function_key,
                    "provider": model_provider,
                    "model": model,
                    "duration": (datetime.now() - start_time).total_seconds(),
                    "input_length": len(user_message),
                    "output_length": len(str(error_result))
                }
            
        elif is_image_model:
            # Pro ostatn√≠ image modely (pokud nƒõjak√© jsou)
            logger.info(f"üé® OBECN√ù IMAGE GENERATION: model={model}")
            
            # T≈ôi pokusy pro image generation
            llm_result = None
            last_error = None
            
            for attempt in range(3):
                try:
                    activity.heartbeat()
                    logger.info(f"ü§ñ IMAGE pokus {attempt + 1}/3: {model_provider}/{model}")
                    
                    llm_result = await llm_client.image_generation(
                        prompt=user_message,
                        size="1024x1024",
                        quality="standard", 
                        style="vivid"
                    )
                    
                    if llm_result and "content" in llm_result:
                        logger.info(f"‚úÖ IMAGE √∫spƒõch: {model_provider}/{model}")
                        break
                    else:
                        raise Exception(f"Image generation vr√°til nevalidn√≠ response: {llm_result}")
                        
                except Exception as e:
                    last_error = e
                    logger.warning(f"‚ö†Ô∏è IMAGE pokus {attempt + 1} selhal: {str(e)}")
                    if attempt < 2:
                        await asyncio.sleep(2 ** attempt)
            
            if not llm_result or "content" not in llm_result:
                raise Exception(f"Image generation selhalo po 3 pokusech. Posledn√≠ chyba: {last_error}")
        else:
            # Pro text modely pou≈æ√≠v√°me chat_completion API
            llm_result = await safe_llm_call(
                llm_func=llm_client.chat_completion,
                provider=model_provider,
                model=model,
                system_prompt=system_prompt,
                user_message=user_message,
                temperature=temperature,
                max_tokens=max_tokens,
                max_retries=3
            )
        
        duration = (datetime.now() - start_time).total_seconds()
        
        # Extraha odpovƒõƒè
        content = llm_result.get("content", "")
        if not content:
            raise ValueError(f"LLM vr√°til pr√°zdn√Ω content: {llm_result}")
        
        log_llm_response(logger, model_provider, len(content), duration)
        
        # Speci√°ln√≠ handling pro konkr√©tn√≠ asistenty (kromƒõ ImageRenderer kter√Ω m√° vlastn√≠ handling v√Ω≈°e)
        logger.info(f"üîß PRE-PROCESSING: {function_key} content preview: {content[:100]}...")
        processed_output = await _process_assistant_output(
            function_key, content, assistant_name
        )
        logger.info(f"üîß POST-PROCESSING: {function_key} output preview: {processed_output[:100]}...")
        
        return {
            "status": "completed",
            "output": processed_output,
            "assistant": assistant_name,
            "function_key": function_key,
            "provider": model_provider,
            "model": model,
            "duration": duration,
            "input_length": len(user_message),
            "output_length": len(processed_output)
        }
        
    except Exception as e:
        logger.error(f"‚ùå LLM vol√°n√≠ selhalo pro {assistant_name}: {str(e)}")
        raise

async def _extract_image_prompts_from_input(user_message: str) -> list:
    """
    Extrahuje image_prompts z v√Ωstupu MultimediaAssistant.
    
    Args:
        user_message: Input pro ImageRenderer (obsahuje output od MultimediaAssistant)
        
    Returns:
        Seznam image_prompts jako List[str]
    """
    import json
    import re
    
    try:
        logger.info(f"üîç Extrakce image_prompts z input: {user_message[:200]}...")
        
        # Zkus√≠me naj√≠t JSON s image_prompts v user_message
        # MultimediaAssistant vrac√≠ output jako JSON string s image_prompts
        
        # Metoda 1: Hled√°me JSON p≈ô√≠mo
        try:
            # Mo≈æn√° je to cel√© JSON
            data = json.loads(user_message)
            if isinstance(data, list) and len(data) > 0:
                # Je to seznam prompt≈Ø p≈ô√≠mo
                logger.info(f"üé® Na≈°el jsem seznam prompt≈Ø p≈ô√≠mo: {len(data)} items")
                return [str(item) for item in data if str(item).strip()]
            elif isinstance(data, dict):
                # üîß NOV√Å LOGIKA: Podpora primary_visuals + optional_visuals struktury
                all_prompts = []
                
                if "image_prompts" in data:
                    prompts = data["image_prompts"]
                    logger.info(f"üé® Na≈°el jsem image_prompts v dict: {len(prompts)} items")
                    all_prompts.extend([str(prompt) for prompt in prompts if str(prompt).strip()])
                
                # Podpora nov√© struktury z MultimediaAssistant
                if "primary_visuals" in data:
                    for visual in data["primary_visuals"]:
                        if "image_prompt" in visual:
                            all_prompts.append(visual["image_prompt"])
                    logger.info(f"üé® Na≈°el jsem {len(data['primary_visuals'])} primary_visuals prompt≈Ø")
                
                if "optional_visuals" in data:
                    for visual in data["optional_visuals"]:
                        if "image_prompt" in visual:
                            all_prompts.append(visual["image_prompt"])
                    logger.info(f"üé® Na≈°el jsem {len(data['optional_visuals'])} optional_visuals prompt≈Ø")
                
                if all_prompts:
                    logger.info(f"üé® CELKEM nalezeno {len(all_prompts)} image_prompts")
                    return [prompt.strip() for prompt in all_prompts if prompt.strip()]
        except json.JSONDecodeError:
            pass
        
        # Metoda 2: Hled√°me JSON uvnit≈ô textu pomoc√≠ regex
        json_pattern = r'\[\s*"[^"]+(?:"\s*,\s*"[^"]+)*"\s*\]'
        json_matches = re.findall(json_pattern, user_message, re.DOTALL)
        
        for match in json_matches:
            try:
                prompts = json.loads(match)
                if isinstance(prompts, list) and len(prompts) > 0:
                    valid_prompts = [str(prompt) for prompt in prompts if str(prompt).strip()]
                    if valid_prompts:
                        logger.info(f"üé® Na≈°el jsem {len(valid_prompts)} prompt≈Ø pomoc√≠ regex")
                        return valid_prompts
            except:
                continue
        
        # Metoda 3: Hled√°me prompty jako citovan√© ≈ôetƒõzce
        quote_pattern = r'"([^"]{20,})"'
        quote_matches = re.findall(quote_pattern, user_message)
        
        if quote_matches:
            # Filtruj jen ty co vypadaj√≠ jako image prompty (aspo≈à 20 znak≈Ø)
            valid_prompts = [match for match in quote_matches if len(match) >= 20]
            if valid_prompts:
                logger.info(f"üé® Na≈°el jsem {len(valid_prompts)} prompt≈Ø v uvozovk√°ch")
                return valid_prompts
        
        logger.warning(f"‚ö†Ô∏è ≈Ω√°dn√© image_prompts nenalezeny v input")
        logger.warning(f"‚ö†Ô∏è Input sample: {user_message[:500]}")
        return []
        
    except Exception as e:
        logger.error(f"‚ùå Chyba p≈ôi extrakci image_prompts: {e}")
        return []

async def _extract_url_from_dalle_response(dalle_response: dict) -> str:
    """
    Extrahuje URL z DALL-E API response.
    
    Args:
        dalle_response: Response z DALL-E API
        
    Returns:
        URL obr√°zku nebo None
    """
    try:
        logger.info(f"üîç Extrakce URL z DALL-E response: {type(dalle_response)}")
        
        if isinstance(dalle_response, dict):
            # Zkus content.url structure
            if "content" in dalle_response:
                content = dalle_response["content"]
                if isinstance(content, str):
                    # Je to string s URL - extrahni pomoc√≠ regex
                    import re
                    urls = re.findall(r'https://[^\s\n\)]+', content)
                    if urls:
                        logger.info(f"‚úÖ URL nalezena v content string: {urls[0][:100]}...")
                        return urls[0]
                elif isinstance(content, list) and len(content) > 0:
                    # Je to seznam s objekty
                    item = content[0]
                    if isinstance(item, dict) and "url" in item:
                        logger.info(f"‚úÖ URL nalezena v content[0].url: {item['url'][:100]}...")
                        return item["url"]
            
            # Zkus p≈ô√≠mou URL
            if "url" in dalle_response:
                logger.info(f"‚úÖ URL nalezena p≈ô√≠mo: {dalle_response['url'][:100]}...")
                return dalle_response["url"]
        
        logger.warning(f"‚ö†Ô∏è ≈Ω√°dn√° URL nenalezena v DALL-E response")
        logger.warning(f"‚ö†Ô∏è Response structure: {dalle_response}")
        return None
        
    except Exception as e:
        logger.error(f"‚ùå Chyba p≈ôi extrakci URL: {e}")
        return None

async def _process_assistant_output(
    function_key: str, 
    raw_output: str, 
    assistant_name: str
) -> str:
    """
    Zpracuje v√Ωstup asistenta podle jeho typu.
    ImageRenderer u≈æ m√° vlastn√≠ speci√°ln√≠ handling, tak≈æe tahle funkce se u≈æ nepou≈æ√≠v√°.
    """
    # Pro v≈°echny asistenty vr√°t√≠me surov√Ω v√Ωstup
    return raw_output.strip()

# üóëÔ∏è ODSTRANƒöNO: _process_image_renderer_output - ImageRenderer m√° teƒè vlastn√≠ speci√°ln√≠ handling