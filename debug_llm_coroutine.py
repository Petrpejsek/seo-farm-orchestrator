#!/usr/bin/env python3

"""
Debug script pro testov√°n√≠ LLM coroutine probl√©mu
"""

import asyncio
import sys
import os

# P≈ôid√°me root do sys.path
sys.path.insert(0, os.path.abspath('.'))

from backend.llm_clients.factory import LLMClientFactory

async def test_llm_coroutine_problem():
    """Test probl√©mu s coroutine v LLM vol√°n√≠"""
    
    # Nastav√≠me API_BASE_URL
    os.environ['API_BASE_URL'] = 'http://localhost:8000'
    
    try:
        print("üîß Test 1: Vytvo≈ôen√≠ LLM client")
        llm_client = LLMClientFactory.create_client("openai")
        print(f"‚úÖ LLM client vytvo≈ôen: {type(llm_client)}")
        
        print("üîß Test 2: Z√≠sk√°n√≠ chat_completion funkce")
        llm_func = llm_client.chat_completion
        print(f"‚úÖ chat_completion func: {type(llm_func)}, {llm_func}")
        
        print("üîß Test 3: Vol√°n√≠ chat_completion s await")
        result = await llm_func(
            system_prompt="Jsi pomocn√≠k",
            user_message="≈ò√≠kej ahoj",
            model="gpt-4o-mini",
            temperature=0.7,
            max_tokens=50
        )
        
        print(f"‚úÖ V√Ωsledek typu: {type(result)}")
        
        print("üîß Test 4: Kontrola 'content' in result")
        if "content" in result:
            print(f"‚úÖ 'content' nalezen v result: {result['content'][:50]}...")
        else:
            print(f"‚ùå 'content' NENALEZEN v result: {result}")
            
    except Exception as e:
        print(f"‚ùå CHYBA: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_llm_coroutine_problem())