"""
Centralizovan√Ω OpenAI client s auditovan√Ωmi parametry pro SEO Farm Orchestrator.

Tento modul poskytuje v≈°echny OpenAI API vol√°n√≠ s p≈ôesnƒõ definovan√Ωmi parametry
podle fin√°ln√≠ specifikace. ≈Ω√°dn√© defaultn√≠ hodnoty nejsou ponech√°ny!
"""

import json
import logging
import os
from typing import Optional, Dict, Any, List
from openai import OpenAI
import asyncio
# Import BaseLLMClient bez circular dependency
try:
    from backend.llm_clients.base import BaseLLMClient
except ImportError:
    import sys
    sys.path.append(os.path.dirname(__file__))
    from llm_clients.base import BaseLLMClient

logger = logging.getLogger(__name__)

# üéõÔ∏è FIN√ÅLN√ç AUDITOVAN√â OPENAI PARAMETRY - ZCELA BEZ DEFAULT≈Æ!
OPENAI_CONFIG = {
    "model": "gpt-4o",  # JEDIN√ù MODEL - ≈æ√°dn√© fallbacky
    "temperature": 0.7,  # Kreativita vs konzistence
    "max_tokens": None,  # Bez omezen√≠ - nech√°v√°me na OpenAI API
    "top_p": 1.0,  # Nepou≈æ√≠vat souƒçasnƒõ s temperature
    "frequency_penalty": 0.0,  # ≈Ω√°dn√© penalty - explicitnƒõ 0.0
    "presence_penalty": 0.0,  # ≈Ω√°dn√© penalty - explicitnƒõ 0.0
    "timeout": 120,  # 2 minuty timeout pro API vol√°n√≠
    "request_timeout": 60  # Timeout pro jednotliv√© request
}

# üé® DALL¬∑E 3 PARAMETRY PRO IMAGE GENERATION
DALLE_CONFIG = {
    "model": "dall-e-3",  # Pouze nejnovƒõj≈°√≠ model
    "size": "1024x1024",  # Standardn√≠ velikost
    "quality": "standard",  # standard nebo hd
    "style": "natural",  # natural nebo vivid  
    "n": 1,  # Poƒçet obr√°zk≈Ø (DALL¬∑E 3 podporuje pouze n=1)
    "timeout": 180  # 3 minuty pro image generation
}

def get_api_key(service: str = "openai") -> str:
    """
    Naƒçte API kl√≠ƒç pro OpenAI z backend API nebo environment variables.
    
    Args:
        service: N√°zev slu≈æby (defaultnƒõ "openai")
        
    Returns:
        API kl√≠ƒç pro OpenAI
        
    Raises:
        Exception: Pokud API kl√≠ƒç nen√≠ nalezen
    """
    try:
        # Pokus o naƒçten√≠ z backend API
        import requests
        backend_url = os.getenv("API_BASE_URL", "http://localhost:8000")
        response = requests.get(f"{backend_url}/api-keys/{service}", timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            api_key = data.get("api_key")
            if api_key and api_key != "Not found":
                logger.info(f"‚úÖ OpenAI API kl√≠ƒç naƒçten z backend API")
                return api_key
        
        logger.error(f"‚ùå Backend API nevr√°tilo platn√Ω kl√≠ƒç")
        raise Exception("‚ùå OpenAI API kl√≠ƒç nenalezen v backend API")
    except Exception as e:
        logger.error(f"‚ùå Chyba p≈ôi naƒç√≠t√°n√≠ API kl√≠ƒçe z backend: {e}")
        raise Exception(f"‚ùå OpenAI API kl√≠ƒç nen√≠ dostupn√Ω: {e}")

class OpenAIClient(BaseLLMClient):
    """
    Centralizovan√Ω OpenAI client s auditovan√Ωmi parametry.
    V≈°echny API vol√°n√≠ proch√°zej√≠ p≈ôes tuto t≈ô√≠du s p≈ôesn√Ωmi parametry.
    """
    
    def __init__(self, api_key: str):
        """Inicializace OpenAI clienta s API kl√≠ƒçem."""
        super().__init__(api_key)
        self.api_key = api_key
        self.client = OpenAI(
            api_key=self.api_key,
            timeout=OPENAI_CONFIG["timeout"]
        )
        logger.info(f"ü§ñ OpenAI client inicializov√°n s auditem parametr≈Ø")
        self._log_config()
    
    def get_supported_models(self) -> Dict[str, List[str]]:
        """Vr√°t√≠ seznam podporovan√Ωch OpenAI model≈Ø."""
        return {
            "text": ["gpt-4o", "gpt-4", "gpt-3.5-turbo"],
            "image": ["dall-e-3", "dall-e-2"]
        }
    
    def get_supported_parameters(self) -> List[str]:
        """Vr√°t√≠ seznam podporovan√Ωch parametr≈Ø."""
        return ["temperature", "max_tokens", "top_p", "system_prompt"]
    
    def validate_model(self, model: str) -> bool:
        """Ovƒõ≈ô√≠ zda je model podporov√°n."""
        all_models = []
        for models in self.get_supported_models().values():
            all_models.extend(models)
        return model in all_models
    
    def _log_config(self):
        """Loguje aktu√°ln√≠ konfiguraci pro audit."""
        logger.info(f"üìä OPENAI CONFIG AUDIT:")
        logger.info(f"   Model: {OPENAI_CONFIG['model']} (STRICT MODE - ≈æ√°dn√© fallbacky)")
        logger.info(f"   Temperature: {OPENAI_CONFIG['temperature']}")
        logger.info(f"   Max tokens: {OPENAI_CONFIG['max_tokens']}")
        logger.info(f"   Top_p: {OPENAI_CONFIG['top_p']}")
        logger.info(f"   Frequency penalty: {OPENAI_CONFIG['frequency_penalty']}")
        logger.info(f"   Presence penalty: {OPENAI_CONFIG['presence_penalty']}")
        logger.info(f"   Timeout: {OPENAI_CONFIG['timeout']}s")
    
    async def chat_completion(
        self, 
        system_prompt: str, 
        user_message: str, 
        model: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Centralizovan√© vol√°n√≠ OpenAI Chat Completion API s auditovan√Ωmi parametry.
        
        Args:
            system_prompt: System prompt pro asistenta
            user_message: U≈æivatelsk√° zpr√°va
            model: Override pro model (optional)
            **kwargs: Dodateƒçn√© parametry (budou logov√°ny ale ne pou≈æity!)
            
        Returns:
            Dict s v√Ωsledkem a metadata
        """
        # Model selection s fallback
        model_to_use = model or OPENAI_CONFIG["model"]
        
        # Varov√°n√≠ p≈ôed dodateƒçn√Ωmi parametry
        if kwargs:
            logger.warning(f"‚ö†Ô∏è Ignoruji nepodporovan√© parametry: {list(kwargs.keys())}")
        
        logger.info(f"ü§ñ CHAT_COMPLETION: model={model_to_use}")
        logger.info(f"üìù PROMPT_LENGTH: system={len(system_prompt)}, user={len(user_message)}")
        
        try:
            # P≈ôiprav√≠me parametry
            api_params = {
                "model": model_to_use,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                "temperature": OPENAI_CONFIG["temperature"],
                "top_p": OPENAI_CONFIG["top_p"],
                "frequency_penalty": OPENAI_CONFIG["frequency_penalty"],
                "presence_penalty": OPENAI_CONFIG["presence_penalty"]
            }
            
            # P≈ôid√°me max_tokens pouze pokud je specifikov√°no
            if OPENAI_CONFIG["max_tokens"] is not None:
                api_params["max_tokens"] = OPENAI_CONFIG["max_tokens"]
            
            response = self.client.chat.completions.create(**api_params)
            
            result = {
                "content": response.choices[0].message.content,
                "model": response.model,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                    "total_tokens": response.usage.total_tokens if response.usage else 0
                },
                "config_used": OPENAI_CONFIG.copy(),
                "timestamp": asyncio.get_event_loop().time() if asyncio.get_event_loop().is_running() else None
            }
            
            logger.info(f"‚úÖ CHAT_COMPLETION √∫spƒõ≈°n√Ω: {result['usage']['total_tokens']} token≈Ø")
            return result
            
        except Exception as e:
            # STRICT MODE - ≈æ√°dn√© fallbacky
            logger.error(f"‚ùå Chat completion selhalo s modelem {model_to_use}: {str(e)}")
            raise
    
    async def image_generation(
        self, 
        prompt: str, 
        size: Optional[str] = None,
        quality: Optional[str] = None,
        style: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Centralizovan√© vol√°n√≠ DALL¬∑E API pro generov√°n√≠ obr√°zk≈Ø.
        
        Args:
            prompt: Prompt pro generov√°n√≠ obr√°zku
            size: Velikost obr√°zku (default z DALLE_CONFIG)
            quality: Kvalita obr√°zku (default z DALLE_CONFIG)
            style: Styl obr√°zku (default z DALLE_CONFIG)
            **kwargs: Dodateƒçn√© parametry (budou logov√°ny ale ne pou≈æity!)
            
        Returns:
            Dict s v√Ωsledkem a metadata
        """
        # Pou≈æit√≠ auditovan√Ωch parametr≈Ø nebo default≈Ø
        size_to_use = size or DALLE_CONFIG["size"]
        quality_to_use = quality or DALLE_CONFIG["quality"]
        style_to_use = style or DALLE_CONFIG["style"]
        
        # Varov√°n√≠ p≈ôed dodateƒçn√Ωmi parametry
        if kwargs:
            logger.warning(f"‚ö†Ô∏è Ignoruji nepodporovan√© parametry: {list(kwargs.keys())}")
        
        # Limitace d√©lky promptu pro DALL¬∑E (max 1000 chars)
        if len(prompt) > 1000:
            prompt = prompt[:997] + "..."
            logger.warning(f"‚ö†Ô∏è Prompt zkr√°cen na 1000 znak≈Ø")
        
        logger.info(f"üé® IMAGE_GENERATION: size={size_to_use}, quality={quality_to_use}, style={style_to_use}")
        logger.info(f"üìù PROMPT_LENGTH: {len(prompt)} chars")
        
        try:
            response = self.client.images.generate(
                model=DALLE_CONFIG["model"],
                prompt=prompt,
                n=DALLE_CONFIG["n"],
                size=size_to_use,
                quality=quality_to_use,
                style=style_to_use
            )
            
            # Vytvo≈ô√≠ content string pro workflow kompatibilitu  
            images_data = [
                {
                    "url": img.url,
                    "revised_prompt": img.revised_prompt
                } for img in response.data
            ]
            
            # Content pro workflow - JSON string s obr√°zky
            content = f"Vygenerov√°no {len(images_data)} obr√°zk≈Ø:\n"
            for i, img in enumerate(images_data, 1):
                content += f"{i}. {img['url']}\n   Prompt: {img['revised_prompt']}\n"
            
            result = {
                "content": content,  # üö® REQUIRED kl√≠ƒç pro workflow
                "images": images_data,
                "model": DALLE_CONFIG["model"],
                "config_used": {
                    "size": size_to_use,
                    "quality": quality_to_use,
                    "style": style_to_use,
                    "n": DALLE_CONFIG["n"]
                },
                "original_prompt": prompt,
                "timestamp": asyncio.get_event_loop().time() if asyncio.get_event_loop().is_running() else None
            }
            
            logger.info(f"‚úÖ IMAGE_GENERATION √∫spƒõ≈°n√Ω: {len(result['images'])} obr√°zk≈Ø")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Image generation selhalo: {str(e)}")
            raise

# Global instance pro jednoduch√© pou≈æit√≠
_openai_client = None

def get_openai_client() -> OpenAIClient:
    """Vr√°t√≠ glob√°ln√≠ instanci OpenAI clienta (singleton pattern)."""
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAIClient()
    return _openai_client

# Convenience funkce pro p≈ô√≠m√© pou≈æit√≠
def call_openai_chat(system_prompt: str, user_message: str, model: Optional[str] = None) -> Dict[str, Any]:
    """Convenience funkce pro chat completion."""
    client = get_openai_client()
    return client.chat_completion(system_prompt, user_message, model)

def call_openai_image(prompt: str, **kwargs) -> Dict[str, Any]:
    """Convenience funkce pro image generation."""
    client = get_openai_client()
    return client.image_generation(prompt, **kwargs)