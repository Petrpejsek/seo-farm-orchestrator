"""
CentralizovanÃ½ OpenAI client s auditovanÃ½mi parametry pro SEO Farm Orchestrator.

Tento modul poskytuje vÅ¡echny OpenAI API volÃ¡nÃ­ s pÅ™esnÄ› definovanÃ½mi parametry
podle finÃ¡lnÃ­ specifikace. Å½Ã¡dnÃ© defaultnÃ­ hodnoty nejsou ponechÃ¡ny!
"""

import json
import logging
import os
from typing import Optional, Dict, Any, List
from openai import OpenAI
import asyncio
# Import BaseLLMClient bez circular dependency
try:
    from backend.llm_clients.base import BaseLLMClient
except ImportError:
    import sys
    sys.path.append(os.path.dirname(__file__))
    from llm_clients.base import BaseLLMClient

logger = logging.getLogger(__name__)

# ðŸŽ›ï¸ FINÃLNÃ AUDITOVANÃ‰ OPENAI PARAMETRY - ZCELA BEZ DEFAULTÅ®!
OPENAI_CONFIG = {
    "model": "gpt-4o",  # JEDINÃ MODEL - Å¾Ã¡dnÃ© fallbacky
    "temperature": 0.7,  # Kreativita vs konzistence
    "max_tokens": None,  # Bez omezenÃ­ - nechÃ¡vÃ¡me na OpenAI API
    "top_p": 1.0,  # NepouÅ¾Ã­vat souÄasnÄ› s temperature
    "frequency_penalty": 0.0,  # Å½Ã¡dnÃ© penalty - explicitnÄ› 0.0
    "presence_penalty": 0.0,  # Å½Ã¡dnÃ© penalty - explicitnÄ› 0.0
    "timeout": 120,  # 2 minuty timeout pro API volÃ¡nÃ­
    "request_timeout": 60  # Timeout pro jednotlivÃ© request
}

# ðŸŽ¨ DALLÂ·E 3 PARAMETRY PRO IMAGE GENERATION
DALLE_CONFIG = {
    "model": "dall-e-3",  # Pouze nejnovÄ›jÅ¡Ã­ model
    "size": "1024x1024",  # StandardnÃ­ velikost
    "quality": "standard",  # standard nebo hd
    "style": "natural",  # natural nebo vivid  
    "n": 1,  # PoÄet obrÃ¡zkÅ¯ (DALLÂ·E 3 podporuje pouze n=1)
    "timeout": 180  # 3 minuty pro image generation
}

def get_api_key(service: str = "openai") -> str:
    """
    NaÄte API klÃ­Ä pro OpenAI z backend API nebo environment variables.
    
    Args:
        service: NÃ¡zev sluÅ¾by (defaultnÄ› "openai")
        
    Returns:
        API klÃ­Ä pro OpenAI
        
    Raises:
        Exception: Pokud API klÃ­Ä nenÃ­ nalezen
    """
    try:
        # Pokus o naÄtenÃ­ z backend API
        import requests
        backend_url = os.getenv("API_BASE_URL", "http://localhost:8000")
        response = requests.get(f"{backend_url}/api-keys/{service}", timeout=5)
        
        if response.status_code == 200:
            data = response.json()
            api_key = data.get("api_key")
            if api_key and api_key != "Not found":
                logger.info(f"âœ… OpenAI API klÃ­Ä naÄten z backend API")
                return api_key
        
        logger.error(f"âŒ Backend API nevrÃ¡tilo platnÃ½ klÃ­Ä")
        raise Exception("âŒ OpenAI API klÃ­Ä nenalezen v backend API")
    except Exception as e:
        logger.error(f"âŒ Chyba pÅ™i naÄÃ­tÃ¡nÃ­ API klÃ­Äe z backend: {e}")
        raise Exception(f"âŒ OpenAI API klÃ­Ä nenÃ­ dostupnÃ½: {e}")

class OpenAIClient(BaseLLMClient):
    """
    CentralizovanÃ½ OpenAI client s auditovanÃ½mi parametry.
    VÅ¡echny API volÃ¡nÃ­ prochÃ¡zejÃ­ pÅ™es tuto tÅ™Ã­du s pÅ™esnÃ½mi parametry.
    """
    
    def __init__(self, api_key: str):
        """Inicializace OpenAI clienta s API klÃ­Äem."""
        super().__init__(api_key)
        self.api_key = api_key
        self.client = OpenAI(
            api_key=self.api_key,
            timeout=OPENAI_CONFIG["timeout"]
        )
        logger.info(f"ðŸ¤– OpenAI client inicializovÃ¡n s auditem parametrÅ¯")
        self._log_config()
    
    def get_supported_models(self) -> Dict[str, List[str]]:
        """VrÃ¡tÃ­ seznam podporovanÃ½ch OpenAI modelÅ¯."""
        return {
            "text": ["gpt-4o", "gpt-4", "gpt-3.5-turbo"],
            "image": ["dall-e-3", "dall-e-2"]
        }
    
    def get_supported_parameters(self) -> List[str]:
        """VrÃ¡tÃ­ seznam podporovanÃ½ch parametrÅ¯."""
        return ["temperature", "max_tokens", "top_p", "system_prompt"]
    
    def validate_model(self, model: str) -> bool:
        """OvÄ›Å™Ã­ zda je model podporovÃ¡n."""
        all_models = []
        for models in self.get_supported_models().values():
            all_models.extend(models)
        return model in all_models
    
    def _log_config(self):
        """Loguje aktuÃ¡lnÃ­ konfiguraci pro audit."""
        logger.info(f"ðŸ“Š OPENAI CONFIG AUDIT:")
        logger.info(f"   Model: {OPENAI_CONFIG['model']} (STRICT MODE - Å¾Ã¡dnÃ© fallbacky)")
        logger.info(f"   Temperature: {OPENAI_CONFIG['temperature']}")
        logger.info(f"   Max tokens: {OPENAI_CONFIG['max_tokens']}")
        logger.info(f"   Top_p: {OPENAI_CONFIG['top_p']}")
        logger.info(f"   Frequency penalty: {OPENAI_CONFIG['frequency_penalty']}")
        logger.info(f"   Presence penalty: {OPENAI_CONFIG['presence_penalty']}")
        logger.info(f"   Timeout: {OPENAI_CONFIG['timeout']}s")
    
    async def chat_completion(
        self, 
        system_prompt: str, 
        user_message: str, 
        model: str,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        CentralizovanÃ© volÃ¡nÃ­ OpenAI Chat Completion API s auditovanÃ½mi parametry.
        
        Args:
            system_prompt: System prompt pro asistenta
            user_message: UÅ¾ivatelskÃ¡ zprÃ¡va
            model: Override pro model (optional)
            **kwargs: DodateÄnÃ© parametry (budou logovÃ¡ny ale ne pouÅ¾ity!)
            
        Returns:
            Dict s vÃ½sledkem a metadata
        """
        # Model selection s fallback
        model_to_use = model or OPENAI_CONFIG["model"]
        
        # VarovÃ¡nÃ­ pÅ™ed dodateÄnÃ½mi parametry
        if kwargs:
            logger.warning(f"âš ï¸ Ignoruji nepodporovanÃ© parametry: {list(kwargs.keys())}")
        
        logger.info(f"ðŸ¤– CHAT_COMPLETION: model={model_to_use}")
        logger.info(f"ðŸ“ PROMPT_LENGTH: system={len(system_prompt)}, user={len(user_message)}")
        
        try:
            # PÅ™ipravÃ­me parametry
            api_params = {
                "model": model_to_use,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                "temperature": OPENAI_CONFIG["temperature"],
                "top_p": OPENAI_CONFIG["top_p"],
                "frequency_penalty": OPENAI_CONFIG["frequency_penalty"],
                "presence_penalty": OPENAI_CONFIG["presence_penalty"]
            }
            
            # PÅ™idÃ¡me max_tokens pouze pokud je specifikovÃ¡no
            if OPENAI_CONFIG["max_tokens"] is not None:
                api_params["max_tokens"] = OPENAI_CONFIG["max_tokens"]
            
            response = self.client.chat.completions.create(**api_params)
            
            result = {
                "content": response.choices[0].message.content,
                "model": response.model,
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens if response.usage else 0,
                    "completion_tokens": response.usage.completion_tokens if response.usage else 0,
                    "total_tokens": response.usage.total_tokens if response.usage else 0
                },
                "config_used": OPENAI_CONFIG.copy(),
                "timestamp": asyncio.get_event_loop().time() if asyncio.get_event_loop().is_running() else None
            }
            
            logger.info(f"âœ… CHAT_COMPLETION ÃºspÄ›Å¡nÃ½: {result['usage']['total_tokens']} tokenÅ¯")
            return result
            
        except Exception as e:
            # STRICT MODE - Å¾Ã¡dnÃ© fallbacky
            logger.error(f"âŒ Chat completion selhalo s modelem {model_to_use}: {str(e)}")
            raise
    
    async def image_generation(
        self, 
        prompt: str, 
        size: Optional[str] = None,
        quality: Optional[str] = None,
        style: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        CentralizovanÃ© volÃ¡nÃ­ DALLÂ·E API pro generovÃ¡nÃ­ obrÃ¡zkÅ¯.
        
        Args:
            prompt: Prompt pro generovÃ¡nÃ­ obrÃ¡zku
            size: Velikost obrÃ¡zku (default z DALLE_CONFIG)
            quality: Kvalita obrÃ¡zku (default z DALLE_CONFIG)
            style: Styl obrÃ¡zku (default z DALLE_CONFIG)
            **kwargs: DodateÄnÃ© parametry (budou logovÃ¡ny ale ne pouÅ¾ity!)
            
        Returns:
            Dict s vÃ½sledkem a metadata
        """
        # PouÅ¾itÃ­ auditovanÃ½ch parametrÅ¯ nebo defaultÅ¯
        size_to_use = size or DALLE_CONFIG["size"]
        quality_to_use = quality or DALLE_CONFIG["quality"]
        style_to_use = style or DALLE_CONFIG["style"]
        
        # VarovÃ¡nÃ­ pÅ™ed dodateÄnÃ½mi parametry
        if kwargs:
            logger.warning(f"âš ï¸ Ignoruji nepodporovanÃ© parametry: {list(kwargs.keys())}")
        
        # Limitace dÃ©lky promptu pro DALLÂ·E (max 1000 chars)
        if len(prompt) > 1000:
            prompt = prompt[:997] + "..."
            logger.warning(f"âš ï¸ Prompt zkrÃ¡cen na 1000 znakÅ¯")
        
        logger.info(f"ðŸŽ¨ IMAGE_GENERATION: size={size_to_use}, quality={quality_to_use}, style={style_to_use}")
        logger.info(f"ðŸ“ PROMPT_LENGTH: {len(prompt)} chars")
        
        try:
            response = self.client.images.generate(
                model=DALLE_CONFIG["model"],
                prompt=prompt,
                n=DALLE_CONFIG["n"],
                size=size_to_use,
                quality=quality_to_use,
                style=style_to_use
            )
            
            # VytvoÅ™Ã­ content string pro workflow kompatibilitu  
            images_data = [
                {
                    "url": img.url,
                    "revised_prompt": img.revised_prompt
                } for img in response.data
            ]
            
            # Content pro workflow - JSON string s obrÃ¡zky
            content = f"VygenerovÃ¡no {len(images_data)} obrÃ¡zkÅ¯:\n"
            for i, img in enumerate(images_data, 1):
                content += f"{i}. {img['url']}\n   Prompt: {img['revised_prompt']}\n"
            
            result = {
                "content": content,  # ðŸš¨ REQUIRED klÃ­Ä pro workflow
                "images": images_data,
                "model": DALLE_CONFIG["model"],
                "config_used": {
                    "size": size_to_use,
                    "quality": quality_to_use,
                    "style": style_to_use,
                    "n": DALLE_CONFIG["n"]
                },
                "original_prompt": prompt,
                "timestamp": asyncio.get_event_loop().time() if asyncio.get_event_loop().is_running() else None
            }
            
            logger.info(f"âœ… IMAGE_GENERATION ÃºspÄ›Å¡nÃ½: {len(result['images'])} obrÃ¡zkÅ¯")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Image generation selhalo: {str(e)}")
            raise

# Global instance pro jednoduchÃ© pouÅ¾itÃ­
_openai_client = None

def get_openai_client() -> OpenAIClient:
    """VrÃ¡tÃ­ globÃ¡lnÃ­ instanci OpenAI clienta (singleton pattern)."""
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAIClient()
    return _openai_client

# Convenience funkce pro pÅ™Ã­mÃ© pouÅ¾itÃ­
def call_openai_chat(system_prompt: str, user_message: str, model: Optional[str] = None) -> Dict[str, Any]:
    """Convenience funkce pro chat completion."""
    client = get_openai_client()
    return client.chat_completion(system_prompt, user_message, model)

def call_openai_image(prompt: str, **kwargs) -> Dict[str, Any]:
    """Convenience funkce pro image generation."""
    client = get_openai_client()
    return client.image_generation(prompt, **kwargs)