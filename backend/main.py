import logging
from typing import Optional
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Query, Path
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from temporal_client import start_seo_pipeline, list_workflows, get_workflow_result, describe_workflow_execution, terminate_workflow

# Import novÃ½ch API routerÅ¯
from api.routes.project import router as project_router
from api.routes.assistant import router as assistant_router
from api.routes.workflow_run import router as workflow_run_router
from api.routes.api_keys import router as api_keys_router

# Import databÃ¡zovÃ©ho pÅ™ipojenÃ­
from api.database import connect_database, disconnect_database

# Import databÃ¡zovÃ©ho pÅ™ipojenÃ­ a workflow run API
from api.database import get_prisma_client

# NastavenÃ­ loggingu
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Lifespan context manager pro startup/shutdown udÃ¡losti
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await connect_database()
    logger.info("âœ… DatabÃ¡ze pÅ™ipojena pÅ™i startu")
    yield
    # Shutdown
    await disconnect_database()
    logger.info("ğŸ”„ DatabÃ¡ze odpojenÃ¡ pÅ™i ukonÄenÃ­")

# FastAPI instance s lifespan
app = FastAPI(
    title="SEO Farm Orchestrator Backend",
    description="FastAPI backend s Temporal.io integracÃ­ pro SEO content generation",
    version="0.1.0",
    lifespan=lifespan
)

# CORS middleware - povolenÃ­ pÅ™Ã­stupu z frontendu
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3001",  # Frontend development server (primary port)
        "http://127.0.0.1:3001",  # Alternative localhost format
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# Pydantic modely pro validaci
class CSVData(BaseModel):
    name: str = Field(..., description="NÃ¡zev CSV souboru")
    content: str = Field(..., description="Base64 encoded obsah CSV souboru")

class PipelineRequest(BaseModel):
    topic: str = Field(..., min_length=1, description="TÃ©ma pro SEO zpracovÃ¡nÃ­")
    project_id: Optional[str] = Field(None, description="ID projektu pro propojenÃ­ workflow")
    csv: Optional[CSVData] = Field(None, description="VolitelnÃ½ CSV soubor")

class PipelineResponse(BaseModel):
    status: str = Field(..., description="Status spuÅ¡tÄ›nÃ­ workflow")
    workflow_id: str = Field(..., description="ID Temporal workflow")
    run_id: str = Field(..., description="Run ID Temporal workflow")
    project_id: Optional[str] = Field(None, description="ID projektu")
    database_id: Optional[str] = Field(None, description="ID zÃ¡znamu v databÃ¡zi")

class TerminateWorkflowRequest(BaseModel):
    reason: str = Field(default="Manually terminated by user", description="DÅ¯vod ukonÄenÃ­ workflow")

# Registrace routerÅ¯
app.include_router(project_router)
app.include_router(assistant_router)
app.include_router(workflow_run_router)
app.include_router(api_keys_router)

# DatabÃ¡zovÃ© pÅ™ipojenÃ­ je nynÃ­ spravovÃ¡no pÅ™es lifespan context manager

@app.get("/")
async def root():
    """Health check endpoint pro ovÄ›Å™enÃ­ stavu API"""
    return {"message": "SEO Farm Orchestrator Backend API", "status": "running"}

@app.post("/api/pipeline-run", response_model=PipelineResponse)
async def pipeline_run(request: PipelineRequest):
    """
    SpustÃ­ SEO pipeline workflow pÅ™es Temporal a vytvoÅ™Ã­ zÃ¡znam v databÃ¡zi.
    
    Args:
        request: Pipeline request s tÃ©matem, project_id a volitelnÃ½m CSV
        
    Returns:
        Response s workflow ID, run ID a databÃ¡zovÃ½m ID
        
    Raises:
        HTTPException: 400 pokud projekt neexistuje, 500 pokud chybÃ­ pÅ™ipojenÃ­ k Temporal
    """
    try:
        logger.info(f"ğŸš€ SpouÅ¡tÃ­m SEO pipeline:")
        logger.info(f"   ğŸ“‹ TÃ©ma: {request.topic}")
        logger.info(f"   ğŸ—ï¸ Project ID: {request.project_id}")
        logger.info(f"   ğŸ“„ CSV: {'âœ… PÅ™iloÅ¾en' if request.csv else 'âŒ Å½Ã¡dnÃ½'}")
        
        # OvÄ›Å™enÃ­ existence projektu pokud je zadÃ¡n project_id
        database_id = None
        if request.project_id:
            try:
                prisma = await get_prisma_client()
                project = await prisma.project.find_unique(where={"id": request.project_id})
                if not project:
                    logger.error(f"âŒ Projekt s ID {request.project_id} nenalezen")
                    raise HTTPException(status_code=400, detail=f"Projekt s ID {request.project_id} neexistuje")
                logger.info(f"âœ… Projekt ovÄ›Å™en: {project.name}")
            except HTTPException:
                raise
            except Exception as e:
                logger.error(f"âŒ Chyba pÅ™i ovÄ›Å™ovÃ¡nÃ­ projektu: {str(e)}")
                raise HTTPException(status_code=500, detail=f"Chyba pÅ™i ovÄ›Å™ovÃ¡nÃ­ projektu: {str(e)}")
        
        # Extrakce CSV obsahu pokud existuje
        csv_base64 = None
        if request.csv:
            csv_base64 = request.csv.content
            logger.info(f"ğŸ“„ CSV soubor pÅ™iloÅ¾en: {request.csv.name}")
        
        # SpuÅ¡tÄ›nÃ­ Temporal workflow
        logger.info("ğŸ”Œ PÅ™ipojuji se k Temporal serveru...")
        workflow_id, run_id = await start_seo_pipeline(
            topic=request.topic,
            project_id=request.project_id,
            csv_base64=csv_base64
        )
        
        logger.info(f"âœ… Temporal workflow ÃºspÄ›Å¡nÄ› spuÅ¡tÄ›n:")
        logger.info(f"   ğŸ†” Workflow ID: {workflow_id}")
        logger.info(f"   ğŸƒ Run ID: {run_id}")
        
        # VytvoÅ™enÃ­ zÃ¡znamu v databÃ¡zi pokud je zadÃ¡n project_id
        if request.project_id:
            try:
                # Import WorkflowRunCreate modelu a create_workflow_run funkce
                from api.routes.workflow_run import WorkflowRunCreate, create_workflow_run
                
                # VytvoÅ™enÃ­ zÃ¡znamu workflow run v databÃ¡zi
                workflow_run_data = WorkflowRunCreate(
                    projectId=request.project_id,
                    topic=request.topic,
                    runId=run_id,
                    workflowId=workflow_id
                )
                
                logger.info(f"ğŸ’¾ UklÃ¡dÃ¡m workflow do databÃ¡ze:")
                logger.info(f"   ğŸ“ Topic: {request.topic}")
                logger.info(f"   ğŸ—ï¸ Project ID: {request.project_id}")
                logger.info(f"   ğŸ†” Workflow ID: {workflow_id}")
                logger.info(f"   ğŸƒ Run ID: {run_id}")
                
                # SkuteÄnÃ© volÃ¡nÃ­ API endpointu pro vytvoÅ™enÃ­ databÃ¡zovÃ©ho zÃ¡znamu
                workflow_response = await create_workflow_run(workflow_run_data)
                database_id = workflow_response.id
                
                logger.info(f"âœ… Workflow run skuteÄnÄ› vytvoÅ™en v databÃ¡zi s ID: {database_id}")
                
            except Exception as e:
                logger.error(f"âš ï¸ Chyba pÅ™i vytvÃ¡Å™enÃ­ databÃ¡zovÃ©ho zÃ¡znamu: {str(e)}")
                logger.info("â„¹ï¸ Workflow pokraÄuje, ale bez databÃ¡zovÃ©ho zÃ¡znamu")
        
        logger.info(f"ğŸ‰ Pipeline ÃºspÄ›Å¡nÄ› spuÅ¡tÄ›na pro tÃ©ma: '{request.topic}'")
        
        return PipelineResponse(
            status="started",
            workflow_id=workflow_id,
            run_id=run_id,
            project_id=request.project_id,
            database_id=database_id
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ KritickÃ¡ chyba pÅ™i spuÅ¡tÄ›nÃ­ pipeline:")
        logger.error(f"   ğŸ“‹ TÃ©ma: {request.topic}")
        logger.error(f"   ğŸ—ï¸ Project ID: {request.project_id}")
        logger.error(f"   ğŸš¨ Chyba: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Chyba pÅ™i spuÅ¡tÄ›nÃ­ workflow: {str(e)}"
        )

@app.get("/api/workflows")
async def get_workflows(limit: int = Query(30, description="MaximÃ¡lnÃ­ poÄet vÃ½sledkÅ¯")):
    """
    NaÄte seznam workflow executions z Temporal serveru.
    
    Args:
        limit: MaximÃ¡lnÃ­ poÄet vÃ½sledkÅ¯ (default 30)
        
    Returns:
        JSON s workflows seznamem
        
    Raises:
        HTTPException: Appropriate HTTP status based on error type
    """
    logger.info(f"ğŸ§  Dotaz na Temporal: naÄÃ­tÃ¡m {limit} workflows...")
    
    try:
        # VolÃ¡nÃ­ funkce pro naÄtenÃ­ workflows
        workflows = await list_workflows(limit=limit)
        
        if not workflows:
            logger.info("ğŸ“­ Å½Ã¡dnÃ© workflows nenalezeny - vrÃ¡cÃ­m prÃ¡zdnÃ© pole")
            return {"workflows": []}
        
        logger.info(f"âœ… VrÃ¡ceno {len(workflows)} workflowÅ¯")
        return {"workflows": workflows}
        
    except ConnectionError as e:
        logger.error(f"âŒ Temporal server nedostupnÃ½: {str(e)}")
        raise HTTPException(
            status_code=503,
            detail="Temporal server je momentÃ¡lnÄ› nedostupnÃ½. Zkuste to znovu pozdÄ›ji."
        )
    except ValueError as e:
        logger.error(f"âŒ NeplatnÃ¡ data z Temporal: {str(e)}")
        raise HTTPException(
            status_code=422,
            detail=f"Chyba zpracovÃ¡nÃ­ dat: {str(e)}"
        )
    except Exception as e:
        # Loguj celÃ½ stacktrace pro debug
        import traceback
        logger.error(f"âŒ NeoÄekÃ¡vÃ¡nÃ¡ chyba pÅ™i naÄÃ­tÃ¡nÃ­ workflows:")
        logger.error(f"   Typ: {type(e).__name__}")
        logger.error(f"   ZprÃ¡va: {str(e)}")
        logger.error(f"   Stacktrace: {traceback.format_exc()}")
        
        raise HTTPException(
            status_code=500,
            detail=f"Chyba pÅ™i naÄÃ­tÃ¡nÃ­ workflows: {str(e)}"
        )

@app.get("/api/workflow-result/{workflow_id}/{run_id}")
async def get_workflow_result_endpoint(
    workflow_id: str = Path(..., description="ID workflow"),
    run_id: str = Path(..., description="Run ID workflow")
):
    """
    ZÃ­skÃ¡ vÃ½sledek dokonÄenÃ©ho workflow z Temporal serveru s diagnostickÃ½mi informacemi.
    
    Args:
        workflow_id: ID workflow
        run_id: Run ID workflow
        
    Returns:
        JSON s workflow vÃ½sledkem, metadata a diagnostickÃ½mi informacemi
        
    Raises:
        HTTPException: 404 pokud workflow neexistuje, 503 pokud Temporal nenÃ­ dostupnÃ½
    """
    logger.info(f"ğŸ“¤ Fetch result: workflow_id={workflow_id}, run_id={run_id}")
    
    try:
        # ğŸ”§ OPRAVA: NejdÅ™Ã­v zkusÃ­me naÄÃ­st aktualizovanÃ¡ data z databÃ¡ze
        from api.database import get_prisma_client
        prisma = await get_prisma_client()
        
        # HledÃ¡me workflow v databÃ¡zi
        db_run = await prisma.workflowrun.find_unique(
            where={
                "workflowId_runId": {
                    "workflowId": workflow_id,
                    "runId": run_id
                }
            }
        )
        

        
        # Pokud mÃ¡me uloÅ¾enÃ¡ aktualizovanÃ¡ data z retry, pouÅ¾ijeme je
        if db_run and db_run.resultJson:
            try:
                import json
                result_data = json.loads(db_run.resultJson)

                logger.info("âœ… NaÄtena aktualizovanÃ¡ data z databÃ¡ze (vÄetnÄ› retry zmÄ›n)")
            except Exception as e:
                # Fallback na Temporal pokud JSON parsing selÅ¾e
                result_data = await get_workflow_result(workflow_id=workflow_id, run_id=run_id)
                logger.warning(f"JSON parsing failed, using Temporal data: {e}")
        else:
            # NaÄteme z Temporal a aktualizujeme databÃ¡zi
            logger.info("No database data found, loading from Temporal")
            result_data = await get_workflow_result(workflow_id=workflow_id, run_id=run_id)
            await update_workflow_status_in_database(workflow_id=workflow_id, run_id=run_id, result_data=result_data)
            logger.info("âœ… NaÄtena fresh data z Temporal")
        
        # PÅ™idÃ¡me diagnostickÃ© informace pro RUNNING i TIMED_OUT workflow  
        if result_data.get("status") in ["RUNNING", "TIMED_OUT", "FAILED"]:
            try:
                diagnostic_info = await describe_workflow_execution(workflow_id=workflow_id, run_id=run_id)
                
                # SlouÄÃ­me diagnostickÃ© informace s vÃ½sledkem
                result_data.update({
                    "current_phase": diagnostic_info.get("current_phase", "Unknown"),
                    "current_activity_type": diagnostic_info.get("current_activity_type"),
                    "elapsed_seconds": diagnostic_info.get("elapsed_seconds", 0),
                    "activity_elapsed_seconds": diagnostic_info.get("activity_elapsed_seconds", 0),
                    "activity_attempt": diagnostic_info.get("activity_attempt", 0),
                    "is_long_running": diagnostic_info.get("is_long_running", False),
                    "warning": diagnostic_info.get("warning", False),
                    "workflow_history": diagnostic_info.get("workflow_history", [])  # ğŸ” AUDIT: Historie aktivit
                })
                
                logger.info(f"ğŸ¯ Current phase: {diagnostic_info.get('current_phase')} ({diagnostic_info.get('elapsed_seconds', 0)/60:.1f} min)")
                
            except Exception as diag_error:
                logger.warning(f"âš ï¸ Diagnostika selhala: {str(diag_error)}")
                # PÅ™idÃ¡me alespoÅˆ basic info
                result_data.update({
                    "current_phase": "Unknown (diagnostic failed)",
                    "warning": False,
                    "diagnostic_error": str(diag_error)
                })
        
        logger.info(f"âœ… Result loaded: status={result_data.get('status')}")
        return result_data
        
    except ValueError as e:
        # Workflow neexistuje nebo nenÃ­ dokonÄen
        logger.warning(f"âš ï¸ Workflow nenalezen: {str(e)}")
        raise HTTPException(
            status_code=404,
            detail={
                "error": "Workflow nenalezen nebo vÃ½stup nenÃ­ k dispozici",
                "message": str(e)
            }
        )
    except ConnectionError as e:
        # Temporal server nenÃ­ dostupnÃ½
        logger.error(f"âŒ Temporal connection error: {str(e)}")
        raise HTTPException(
            status_code=503,
            detail={
                "error": "Temporal server nenÃ­ dostupnÃ½",
                "message": "Zkuste to pozdÄ›ji nebo kontaktujte administrÃ¡tora"
            }
        )
    except Exception as e:
        # OstatnÃ­ chyby
        logger.error(f"âŒ Error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "Nastala chyba pÅ™i naÄÃ­tÃ¡nÃ­ vÃ½stupu",
                "message": str(e)
            }
        )

@app.post("/api/workflow-terminate/{workflow_id}/{run_id}")
async def terminate_workflow_endpoint(
    workflow_id: str = Path(..., description="ID workflow"),
    run_id: str = Path(..., description="Run ID workflow"),
    request: TerminateWorkflowRequest = None
):
    """
    UkonÄÃ­ bÄ›Å¾Ã­cÃ­ workflow execution.
    
    Args:
        workflow_id: ID workflow
        run_id: Run ID workflow  
        request: PoÅ¾adavek s dÅ¯vodem ukonÄenÃ­
        
    Returns:
        JSON s potvrzenÃ­m ukonÄenÃ­
        
    Raises:
        HTTPException: 404 pokud workflow neexistuje nebo nenÃ­ RUNNING, 503 pokud Temporal nenÃ­ dostupnÃ½
    """
    reason = request.reason if request else "Manually terminated by user"
    logger.info(f"â›” Terminate request: workflow_id={workflow_id}, run_id={run_id}, reason={reason}")
    
    try:
        result = await terminate_workflow(workflow_id=workflow_id, run_id=run_id, reason=reason)
        logger.info(f"âœ… Workflow terminated successfully: {workflow_id}")
        return result
        
    except ValueError as e:
        # Workflow neexistuje nebo nenÃ­ RUNNING
        logger.warning(f"âš ï¸ Cannot terminate workflow: {str(e)}")
        raise HTTPException(
            status_code=404,
            detail={
                "error": "Workflow nelze ukonÄit",
                "message": str(e)
            }
        )
    except ConnectionError as e:
        # Temporal server nenÃ­ dostupnÃ½
        logger.error(f"âŒ Temporal connection error: {str(e)}")
        raise HTTPException(
            status_code=503,
            detail={
                "error": "Temporal server nenÃ­ dostupnÃ½",
                "message": "Zkuste to pozdÄ›ji nebo kontaktujte administrÃ¡tora"
            }
        )
    except Exception as e:
        # OstatnÃ­ chyby
        logger.error(f"âŒ Error terminating workflow: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "Nastala chyba pÅ™i ukonÄovÃ¡nÃ­ workflow",
                "message": str(e)
            }
        )

async def update_workflow_status_in_database(workflow_id: str, run_id: str, result_data: dict):
    """
    Aktualizuje status workflow v databÃ¡zi na zÃ¡kladÄ› informacÃ­ z Temporal serveru.
    
    Args:
        workflow_id: ID workflow z Temporal
        run_id: Run ID workflow z Temporal 
        result_data: VÃ½sledek z get_workflow_result
    """
    try:
        from api.routes.workflow_run import get_prisma_client
        from datetime import datetime
        

        
        prisma = await get_prisma_client()

        
        # Najdeme workflow run podle workflowId a runId
        existing_run = await prisma.workflowrun.find_unique(
            where={
                "workflowId_runId": {
                    "workflowId": workflow_id,
                    "runId": run_id
                }
            }
        )
        
        if not existing_run:
            logger.warning(f"âš ï¸ Workflow run {workflow_id}/{run_id} nenalezen v databÃ¡zi pro aktualizaci")
            return
        
        # PÅ™ipravÃ­me data pro aktualizaci
        update_fields = {}
        temporal_status = result_data.get("status")
        
        # MapovÃ¡nÃ­ Temporal statusÅ¯ na naÅ¡e databÃ¡zovÃ© statusy
        if temporal_status == "COMPLETED":
            update_fields["status"] = "COMPLETED"
            if result_data.get("end_time"):
                update_fields["finishedAt"] = datetime.fromisoformat(result_data["end_time"].replace('Z', '+00:00'))
        elif temporal_status == "FAILED":
            update_fields["status"] = "FAILED"
            if result_data.get("end_time"):
                update_fields["finishedAt"] = datetime.fromisoformat(result_data["end_time"].replace('Z', '+00:00'))
        elif temporal_status == "TIMED_OUT":
            update_fields["status"] = "TIMED_OUT"
            if result_data.get("end_time"):
                update_fields["finishedAt"] = datetime.fromisoformat(result_data["end_time"].replace('Z', '+00:00'))
        elif temporal_status == "RUNNING":
            update_fields["status"] = "RUNNING"
        else:
            update_fields["status"] = temporal_status or "UNKNOWN"
        
        # PÅ™idÃ¡me vÃ½sledek jako JSON pokud existuje
        if result_data.get("result"):
            import json
            update_fields["resultJson"] = json.dumps(result_data["result"], ensure_ascii=False)
        
        # PÅ™idÃ¡me stage informace pokud existujÃ­
        if result_data.get("stage_logs"):
            completed_stages = len([log for log in result_data["stage_logs"] if log.get("status") == "COMPLETED"])
            total_stages = len(result_data["stage_logs"])
            update_fields["stageCount"] = completed_stages
            update_fields["totalStages"] = total_stages
        
        # ğŸ”§ KRITICKÃ‰: UloÅ¾Ã­me celou aktualizovanou strukturu do databÃ¡ze
        # Frontend potÅ™ebuje pÅ™Ã­stup k aktualizovanÃ½m stages po retry
        # PouÅ¾Ã­vÃ¡me existujÃ­cÃ­ pole resultJson mÃ­sto vytvÃ¡Å™enÃ­ novÃ©ho
        update_fields["resultJson"] = json.dumps(result_data, ensure_ascii=False)
        logger.info(f"ğŸ’¾ UklÃ¡dÃ¡m celou workflow strukturu do databÃ¡ze (vÄetnÄ› stages)")
        

        
        # Aktualizace v databÃ¡zi
        updated_run = await prisma.workflowrun.update(
            where={"id": existing_run.id},
            data=update_fields
        )
        
        logger.info(f"âœ… Database updated: {updated_run.status} ({updated_run.stageCount}/{updated_run.totalStages} stages)")
        
    except Exception as e:
        logger.error(f"âŒ Database update failed: {str(e)}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        # Nebudeme hadit exception, aby to nerozhodilo hlavnÃ­ flow


@app.post("/api/retry-publish-script")
async def retry_publish_script(request: dict):
    """
    SpustÃ­ pouze PublishScript bez celÃ© pipeline pro Ãºsporu AI kreditÅ¯
    """
    try:
        workflow_id = request.get("workflow_id")
        run_id = request.get("run_id")
        stage = request.get("stage")
        
        if not workflow_id or not run_id:
            raise HTTPException(status_code=400, detail="workflow_id a run_id jsou povinnÃ©")
        
        logger.info(f"ğŸ”§ Retry PublishScript:")
        logger.info(f"   ğŸ†” URL Workflow ID: {workflow_id}")
        logger.info(f"   ğŸƒ URL Run ID: {run_id}")
        logger.info(f"   ğŸ“‹ Stage: {stage}")
        
        # ğŸ” OPRAVA: NejdÅ™Ã­v resolvujeme sprÃ¡vnÃ© Temporal IDs z databÃ¡ze
        from api.database import get_prisma_client
        
        try:
            prisma = await get_prisma_client()
            
            # HledÃ¡me workflow v databÃ¡zi podle URL parametrÅ¯
            # URL parametry mohou bÃ½t ÄÃ¡st nÃ¡zvu workflow_id, takÅ¾e hledÃ¡me pÅ™es LIKE
            workflow_records = await prisma.workflowrun.find_many(
                where={
                    "OR": [
                        {"workflowId": {"contains": workflow_id}},
                        {"runId": run_id},
                        {"id": run_id}  # moÅ¾nÃ¡ run_id je database ID
                    ]
                }
            )
            
            if not workflow_records:
                logger.error(f"âŒ NenaÅ¡el jsem workflow v databÃ¡zi pro URL parametry")
                raise HTTPException(status_code=404, detail=f"Workflow nenalezen v databÃ¡zi pro zadanÃ© parametry")
            
            # Bereme prvnÃ­ (a pravdÄ›podobnÄ› jedinÃ½) zÃ¡znam
            workflow_record = workflow_records[0]
            actual_workflow_id = workflow_record.workflowId
            actual_run_id = workflow_record.runId
            
            logger.info(f"âœ… ResolvovÃ¡ny sprÃ¡vnÃ© Temporal IDs:")
            logger.info(f"   ğŸ¯ SkuteÄnÃ½ Workflow ID: {actual_workflow_id}")
            logger.info(f"   ğŸ¯ SkuteÄnÃ½ Run ID: {actual_run_id}")
            
        except Exception as db_error:
            logger.error(f"âŒ Chyba pÅ™i hledÃ¡nÃ­ workflow v databÃ¡zi: {db_error}")
            raise HTTPException(status_code=500, detail=f"Chyba pÅ™i hledÃ¡nÃ­ workflow: {str(db_error)}")
        
        # NaÄteme workflow vÃ½sledek pro zÃ­skÃ¡nÃ­ pipeline dat
        from temporal_client import get_workflow_result
        
        workflow_result = await get_workflow_result(actual_workflow_id, actual_run_id)
        
        if not workflow_result or not workflow_result.get("result"):
            raise HTTPException(status_code=404, detail="Workflow data nenalezena")
        
        pipeline_data = workflow_result["result"]
        
        # SpustÃ­me pouze publish_activity s aktuÃ¡lnÃ­mi daty
        from temporalio.client import Client
        import sys
        import os
        sys.path.append(os.path.dirname(os.path.dirname(__file__)))
        from workflows.assistant_pipeline_workflow import AssistantPipelineWorkflow
        
        # PÅ™ipojenÃ­ k Temporal
        client = await Client.connect("localhost:7233")
        
        # DIAGNOSTIKA: Zkontrolujme strukturu dat
        logger.info(f"ğŸ” Pipeline data keys: {list(pipeline_data.keys())}")
        logger.info(f"ğŸ” Pipeline final_output sample: {str(pipeline_data.get('final_output', 'MISSING'))[:300]}...")
        
        # Pipeline data obsahuje stage_logs, ale potÅ™ebujeme finÃ¡lnÃ­ vÃ½stup
        final_output = pipeline_data.get("final_output", "")
        
        if isinstance(final_output, str) and final_output.strip().startswith('{'):
            # Pokud je final_output JSON string, parsujeme ho
            try:
                import json
                final_output = json.loads(final_output)
                logger.info("âœ… Final output ÃºspÄ›Å¡nÄ› parsovÃ¡n jako JSON")
            except:
                logger.warning("âš ï¸ Nelze parsovat final output jako JSON")
        
        # SPRÃVNÃ EXTRAKCE DAT ze stage_logs
        stage_logs = pipeline_data.get("stage_logs", [])
        logger.info(f"ğŸ“Š Nalezeno {len(stage_logs)} stage logs")
        
        # CHRONOLOGICKÃ DIAGNOSTIKA PIPELINE
        logger.info("ğŸ” CHRONOLOGIE PIPELINE:")
        for i, log in enumerate(stage_logs):
            stage = log.get("stage", "UNKNOWN")
            status = log.get("status", "UNKNOWN")
            output_preview = str(log.get("output", ""))[:100] + "..." if log.get("output") else "NO OUTPUT"
            logger.info(f"  {i+1:2d}. {stage} - {status} - {output_preview}")
        
        # Extrakce vÃ½stupÅ¯ asistentÅ¯ ze stage_logs
        components = {
            "draft_assistant_output": "",
            "seo_assistant_output": "",
            "humanizer_assistant_output": "",
            "humanizer_output_after_fact_validation": "",  # âœ… PÅ˜IDÃNO
            "multimedia_assistant_output": "",
            "image_renderer_assistant_output": "",
            "qa_assistant_output": "",
            "fact_validator_assistant_output": "",
            "brief_assistant_output": ""
        }
        
        # Projdeme stage_logs a najdeme vÃ½stupy jednotlivÃ½ch asistentÅ¯
        for log in stage_logs:
            stage_name = log.get("stage", "")
            output = log.get("output", "")
            
            if stage_name and output:
                # ğŸ” DETAILNÃ DEBUG MAPPING
                logger.info(f"ğŸ” ZpracovÃ¡vÃ¡m stage: '{stage_name}' -> output length: {len(str(output))}")
                
                # MapovÃ¡nÃ­ stage names na component keys
                if "seo" in stage_name.lower():
                    components["seo_assistant_output"] = output
                    logger.info(f"âœ… SEO output ÃšSPÄšÅ NÄš namapovÃ¡n: {len(str(output))} znakÅ¯")
                    logger.info(f"ğŸ” SEO output preview: {str(output)[:200]}...")
                elif "draft" in stage_name.lower():
                    components["draft_assistant_output"] = output
                    logger.info(f"âœ… Draft output nalezen: {len(str(output))} znakÅ¯")
                elif "humanizer" in stage_name.lower():
                    components["humanizer_assistant_output"] = output
                    components["humanizer_output_after_fact_validation"] = output  # âœ… PÅ˜IDÃNO - same jako humanizer
                    logger.info(f"âœ… Humanizer output nalezen: {len(str(output))} znakÅ¯")
                    logger.info(f"âœ… Humanizer takÃ© namapovÃ¡n jako humanizer_output_after_fact_validation")
                elif "multimedia" in stage_name.lower():
                    components["multimedia_assistant_output"] = output
                    logger.info(f"âœ… Multimedia output nalezen: {len(str(output))} znakÅ¯")
                elif "image" in stage_name.lower():
                    components["image_renderer_assistant_output"] = output
                    logger.info(f"âœ… Image output nalezen: {len(str(output))} znakÅ¯")
                elif "qa" in stage_name.lower():
                    components["qa_assistant_output"] = output
                    logger.info(f"âœ… QA output nalezen: {len(str(output))} znakÅ¯")
                elif "fact" in stage_name.lower() or "validator" in stage_name.lower():
                    components["fact_validator_assistant_output"] = output
                    logger.info(f"âœ… FactValidator output nalezen: {len(str(output))} znakÅ¯")
                elif "brief" in stage_name.lower():
                    components["brief_assistant_output"] = output
                    logger.info(f"âœ… Brief output nalezen: {len(str(output))} znakÅ¯")
        
        logger.info(f"ğŸ“Š Extrakce dokonÄena: {sum(1 for v in components.values() if v)} neprÃ¡zdnÃ½ch vÃ½stupÅ¯")
        
        # DIRECT SCRIPT CALL - spustÃ­me publish_script pÅ™Ã­mo bez Temporal
        try:
            from datetime import datetime
            import sys
            import os
            import json
            sys.path.append(os.path.dirname(os.path.dirname(__file__)))
            from helpers.transformers import transform_to_PublishInput
            from activities.publish_script import publish_script
            
            logger.info("ğŸ”§ SpouÅ¡tÃ­m PublishScript pÅ™Ã­mo jako Python funkci...")
            logger.info(f"ğŸ“Š Components keys: {list(components.keys())}")
            logger.info(f"ğŸ“Š SEO output sample: {components.get('seo_assistant_output', 'MISSING')[:300]}...")
            logger.info(f"ğŸ“Š Draft output sample: {components.get('draft_assistant_output', 'MISSING')[:200]}...")
            
            # Debug SEO parsovÃ¡nÃ­
            try:
                from helpers.transformers import parse_seo_metadata, parse_qa_faq
                seo_data = parse_seo_metadata(components.get('seo_assistant_output', ''))
                logger.info(f"ğŸ” SEO parsovÃ¡no: title={seo_data.get('title', 'MISSING')}")
                logger.info(f"ğŸ” SEO keywords: {seo_data.get('keywords', [])} (count: {len(seo_data.get('keywords', []))})")
                
                # Debug QA parsovÃ¡nÃ­
                qa_data = parse_qa_faq(components.get('qa_assistant_output', ''))
                logger.info(f"ğŸ” QA parsovÃ¡no: {len(qa_data)} FAQ poloÅ¾ek")
                logger.info(f"ğŸ” QA sample: {components.get('qa_assistant_output', 'MISSING')[:500]}...")
            except Exception as e:
                logger.error(f"âŒ Chyba pÅ™i parsovÃ¡nÃ­: {e}")
            
            # Transform pipeline data na PublishInput format - PÅ˜ED transformacÃ­ pÅ™eveÄ dict na string
            logger.info("ğŸ”§ PÅ™evÃ¡dÃ­m dict objekty na stringy pÅ™ed transformacÃ­...")
            for key, value in components.items():
                if isinstance(value, dict):
                    components[key] = json.dumps(value, ensure_ascii=False)
                    logger.info(f"âœ… {key}: dict pÅ™eveden na JSON string")
                elif not isinstance(value, str):
                    components[key] = str(value)
                    logger.info(f"âœ… {key}: {type(value)} pÅ™eveden na string")
            
            # PÅ™idÃ¡me current_date pro sprÃ¡vnÃ½ ISO 8601 formÃ¡t
            components["current_date"] = components.get("current_date") or __import__('datetime').datetime.now(__import__('datetime').timezone.utc).isoformat().replace('+00:00', 'Z')
            logger.info(f"âœ… current_date nastaven: {components['current_date']}")
            
            publish_input = transform_to_PublishInput(components)
            logger.info(f"ğŸ“Š Transformace dokonÄena: {len(publish_input)} poloÅ¾ek")
            
            # PÅ˜ÃMO SPUSTÃME PUBLISH_SCRIPT
            result = publish_script(publish_input)
            
            # DEBUG: Co publish script vracÃ­?
            logger.info(f"ğŸ” DEBUG: publish_script vrÃ¡til typ: {type(result)}")
            logger.info(f"ğŸ” DEBUG: publish_script keys: {list(result.keys()) if isinstance(result, dict) else 'NOT_DICT'}")
            logger.info(f"ğŸ” DEBUG: success klÃ­Ä: {result.get('success', 'MISSING_KEY') if isinstance(result, dict) else 'NOT_DICT'}")
            logger.info(f"ğŸ” DEBUG: celÃ½ result (prvnÃ­ 500 znakÅ¯): {str(result)[:500]}")
            
            retry_id = f"retry_publish_direct_{workflow_id}_{int(__import__('datetime').datetime.now().timestamp())}"
            logger.info(f"âœ… PublishScript retry dokonÄen: {retry_id}")
            logger.info(f"ğŸ“Š VÃ½sledek: {result.get('success', False)}")
            
            # ğŸ”§ KRITICKÃ OPRAVA: Aktualizuj databÃ¡zi s novÃ½mi regenerovanÃ½mi daty
            logger.info(f"ğŸ” DEBUG: result.get('success') = {result.get('success')}")
            logger.info(f"ğŸ” DEBUG: actual_workflow_id = {actual_workflow_id}")
            logger.info(f"ğŸ” DEBUG: actual_run_id = {actual_run_id}")
            
            if result.get('success'):
                try:
                    logger.info("ğŸ”„ Aktualizuji databÃ¡zi s novÃ½mi regenerovanÃ½mi daty...")
                    
                    # NaÄti aktuÃ¡lnÃ­ workflow data
                    from temporal_client import get_workflow_result
                    logger.info("ğŸ” VolÃ¡m get_workflow_result...")
                    current_workflow_data = await get_workflow_result(actual_workflow_id, actual_run_id)
                    logger.info(f"ğŸ” Workflow data loaded: {bool(current_workflow_data)}")
                    
                    # Debug workflow data structure
                    logger.info(f"ğŸ” Workflow data type: {type(current_workflow_data)}")
                    logger.info(f"ğŸ” Workflow data keys: {list(current_workflow_data.keys()) if isinstance(current_workflow_data, dict) else 'NOT_DICT'}")
                    logger.info(f"ğŸ” Has 'stages' key: {'stages' in current_workflow_data if isinstance(current_workflow_data, dict) else False}")
                    
                    # ğŸ”§ OPRAVA: Stages jsou v 'stage_logs', ne 'stages'
                    stages_key = "stage_logs" if "stage_logs" in current_workflow_data else "stages" 
                    
                    # Najdi PublishScript stage a updatuj jeho output
                    if current_workflow_data and stages_key in current_workflow_data:
                        stages = current_workflow_data[stages_key]
                        logger.info(f"ğŸ” PoÄet {stages_key}: {len(stages)}")
                        
                        # PotÅ™ebujeme pÅ™evÃ©st stage_logs na strukturu kompatibilnÃ­ s frontend
                        # Frontend oÄekÃ¡vÃ¡ 'stages' s 'stage_name' a 'stage_output'
                        updated_stages = []
                        
                        for i, stage_log in enumerate(stages):
                            stage_name = stage_log.get("stage", "")
                            logger.info(f"ğŸ” Stage {i}: {stage_name}")
                            
                            # PÅ™evod stage_log na frontend formÃ¡t
                            stage_frontend = {
                                "stage": stage_name,  # âœ… OPRAVENO: "stage" mÃ­sto "stage_name"
                                "stage_output": stage_log.get("output", ""),
                                "status": stage_log.get("status", ""),
                                "start_time": stage_log.get("start_time", ""),
                                "end_time": stage_log.get("end_time", "")
                            }
                            
                            if "publish" in stage_name.lower():
                                # Aktualizuj PublishScript output s novÃ½mi daty
                                logger.info(f"ğŸ¯ NaÅ¡el jsem PublishScript stage: {stage_name}")
                                stage_frontend["stage_output"] = result
                                stage_frontend["output"] = result  # ğŸ”§ OPRAVA: PÅ™idat takÃ© 'output' field pro frontend tlaÄÃ­tka
                                # âœ… KRITICKÃ OPRAVA: ZmÄ›nÃ­ status na COMPLETED
                                if result.get('success'):
                                    stage_frontend["status"] = "COMPLETED"
                                    logger.info("âœ… PublishScript stage status zmÄ›nÄ›n na COMPLETED")
                                else:
                                    stage_frontend["status"] = "FAILED"
                                    logger.info("âŒ PublishScript stage status zÅ¯stÃ¡vÃ¡ FAILED")
                                logger.info("âœ… PublishScript stage output aktualizovÃ¡n")
                            
                            updated_stages.append(stage_frontend)
                        
                        # Aktualizuj strukturu pro frontend kompatibilitu
                        current_workflow_data["stages"] = updated_stages
                        logger.info(f"âœ… PÅ™evedeno {len(updated_stages)} stages do frontend formÃ¡tu")
                        
                        # ğŸ”§ KRITICKÃ OPRAVA: Synchronizuj stage_logs se stages pro frontend kompatibilitu
                        # Frontend Äte z stage_logs, ale retry aktualizuje stages
                        if "stage_logs" in current_workflow_data:
                            current_workflow_data["stage_logs"] = updated_stages.copy()
                            logger.info(f"âœ… SynchronizovÃ¡ny stage_logs se stages pro frontend")
                        
                        # UloÅ¾ zpÄ›t do databÃ¡ze
                        await update_workflow_status_in_database(actual_workflow_id, actual_run_id, current_workflow_data)
                    else:
                        if not current_workflow_data:
                            logger.warning("âš ï¸ current_workflow_data je falsy")
                        elif "stages" not in current_workflow_data:
                            logger.warning("âš ï¸ 'stages' klÃ­Ä nenÃ­ v current_workflow_data")
                            logger.warning(f"âš ï¸ DostupnÃ© klÃ­Äe: {list(current_workflow_data.keys())}")
                        else:
                            logger.warning("âš ï¸ NeznÃ¡mÃ½ problÃ©m s workflow data")
                        
                except Exception as e:
                    logger.error(f"âŒ Chyba pÅ™i aktualizaci databÃ¡ze: {e}")
                    import traceback
                    logger.error(f"âŒ Traceback: {traceback.format_exc()}")
                    # PokraÄujeme i pÅ™i chybÄ› - hlavnÃ­ vÄ›c je Å¾e retry probÄ›hl
            else:
                logger.warning(f"âš ï¸ Retry result success={result.get('success')} - pÅ™eskakuji update databÃ¡ze")
            
            return {
                "status": "completed" if result.get('success') else "failed",
                "retry_id": retry_id,
                "original_workflow_id": workflow_id,
                "result": result,
                "message": "PublishScript byl dokonÄen" if result.get('success') else "PublishScript selhal"
            }
            
        except Exception as e:
            logger.error(f"âŒ Chyba pÅ™i spuÅ¡tÄ›nÃ­ PublishScript retry: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Chyba pÅ™i spuÅ¡tÄ›nÃ­ retry: {str(e)}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Chyba v retry_publish_script: {str(e)}")
        raise HTTPException(status_code=500, detail=f"NeoÄekÃ¡vanÃ¡ chyba: {str(e)}")

@app.get("/health")
async def health_check():
    """Health check endpoint pro monitoring"""
    return {"status": "healthy", "service": "seo-farm-backend"} 