import logging
from typing import Optional, List
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Query, Path
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from temporal_client import start_seo_pipeline, list_workflows, get_workflow_result, describe_workflow_execution, terminate_workflow

# Import nov√Ωch API router≈Ø
from api.routes.project import router as project_router
from api.routes.assistant import router as assistant_router
from api.routes.workflow_run import router as workflow_run_router
from api.routes.api_keys import router as api_keys_router

# Import datab√°zov√©ho p≈ôipojen√≠
from api.database import connect_database, disconnect_database

# Import datab√°zov√©ho p≈ôipojen√≠ a workflow run API
from api.database import get_prisma_client

# Nastaven√≠ loggingu
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Lifespan context manager pro startup/shutdown ud√°losti
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await connect_database()
    logger.info("‚úÖ Datab√°ze p≈ôipojena p≈ôi startu")
    yield
    # Shutdown
    await disconnect_database()
    logger.info("üîÑ Datab√°ze odpojen√° p≈ôi ukonƒçen√≠")

# FastAPI instance s lifespan
app = FastAPI(
    title="SEO Farm Orchestrator Backend",
    description="FastAPI backend s Temporal.io integrac√≠ pro SEO content generation",
    version="0.1.0",
    lifespan=lifespan
)

# CORS middleware - povolen√≠ p≈ô√≠stupu z frontendu
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Povol√≠ v≈°echny dom√©ny pro debugging
    allow_credentials=False,  # Mus√≠ b√Ωt False kdy≈æ origins=["*"]
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# Pydantic modely pro validaci
class CSVData(BaseModel):
    name: str = Field(..., description="N√°zev CSV souboru")
    content: str = Field(..., description="Base64 encoded obsah CSV souboru")

class PipelineRequest(BaseModel):
    topic: str = Field(..., min_length=1, description="T√©ma pro SEO zpracov√°n√≠")
    project_id: Optional[str] = Field(None, description="ID projektu pro propojen√≠ workflow")
    csv: Optional[CSVData] = Field(None, description="Voliteln√Ω CSV soubor")

class PipelineResponse(BaseModel):
    status: str = Field(..., description="Status spu≈°tƒõn√≠ workflow")
    workflow_id: str = Field(..., description="ID Temporal workflow")
    run_id: str = Field(..., description="Run ID Temporal workflow")
    project_id: Optional[str] = Field(None, description="ID projektu")
    database_id: Optional[str] = Field(None, description="ID z√°znamu v datab√°zi")

class TerminateWorkflowRequest(BaseModel):
    reason: str = Field(default="Manually terminated by user", description="D≈Øvod ukonƒçen√≠ workflow")

# Registrace router≈Ø
app.include_router(project_router)
app.include_router(assistant_router)
app.include_router(workflow_run_router)
app.include_router(api_keys_router)

# Datab√°zov√© p≈ôipojen√≠ je nyn√≠ spravov√°no p≈ôes lifespan context manager

@app.get("/")
async def root():
    """Health check endpoint pro ovƒõ≈ôen√≠ stavu API"""
    return {"message": "SEO Farm Orchestrator Backend API", "status": "running"}

class BatchPipelineRequest(BaseModel):
    """Batch request pro spu≈°tƒõn√≠ v√≠ce workflow z CSV."""
    project_id: str = Field(..., description="ID projektu pro propojen√≠ workflow")
    csv: CSVData = Field(..., description="CSV soubor s t√©maty")
    batch_name: Optional[str] = Field(None, description="N√°zev batch jobu")

class BatchPipelineResponse(BaseModel):
    """Response pro batch spu≈°tƒõn√≠."""
    status: str = Field(..., description="Status spu≈°tƒõn√≠ batch workflow")
    batch_id: str = Field(..., description="ID batch jobu")
    total_workflows: int = Field(..., description="Celkov√Ω poƒçet spu≈°tƒõn√Ωch workflow")
    workflow_ids: list[str] = Field(..., description="Seznam workflow ID")


# ===== üìä LANDING PAGES S TABULKAMI =====

class TableRowModel(BaseModel):
    """Model pro ≈ô√°dek v tabulce"""
    feature: str
    values: list  # R≈Øzn√© typy hodnot
    type: str = "text"  # text, boolean, price, rating, number
    highlight: Optional[list[int]] = None


class ComparisonTableModel(BaseModel):
    """Model pro srovn√°vac√≠ tabulku"""
    type: str = "comparison"
    title: str
    subtitle: Optional[str] = None
    headers: list[str]
    rows: list[TableRowModel]
    highlightColumns: Optional[list[int]] = None
    style: str = "modern"


class PricingTableModel(BaseModel):
    """Model pro cenovou tabulku"""
    type: str = "pricing"
    title: str
    subtitle: Optional[str] = None
    headers: list[str]
    rows: list[TableRowModel]
    highlightColumns: Optional[list[int]] = None
    style: str = "modern"


class FeatureTableModel(BaseModel):
    """Model pro feature tabulku"""
    type: str = "features"
    title: str
    subtitle: Optional[str] = None
    headers: list[str]
    rows: list[TableRowModel]
    style: str = "minimal"


# ===== NOV√â MODELY PODLE SPECIFIKACE =====

class MetaModel(BaseModel):
    """Meta informace pro SEO"""
    description: str = Field(..., description="SEO popis 150-160 znak≈Ø")
    keywords: list[str] = Field(default=[], description="SEO kl√≠ƒçov√° slova")
    ogImage: str = Field(default="", description="URL k hlavn√≠mu obr√°zku")


class VisualsModel(BaseModel):
    """Strukturovan√© vizu√°ln√≠ prvky"""
    comparisonTables: Optional[list[ComparisonTableModel]] = None
    pricingTables: Optional[list[PricingTableModel]] = None
    featureTables: Optional[list[FeatureTableModel]] = None


class LandingPageRequest(BaseModel):
    """Request pro vytvo≈ôen√≠ landing page podle nov√© specifikace"""
    title: str = Field(..., description="P≈ôesn√Ω titulek ƒçl√°nku")
    slug: str = Field(..., description="URL-friendly slug bez diakritiky")
    language: str = Field(default="cs", description="Jazyk obsahu")
    meta: MetaModel = Field(..., description="Meta informace pro SEO")
    contentHtml: str = Field(..., description="HTML obsah ƒçl√°nku")
    visuals: Optional[VisualsModel] = Field(None, description="Strukturovan√© vizu√°ln√≠ prvky")


class LandingPageResponse(BaseModel):
    """Response pro landing page podle nov√© specifikace"""
    id: str = Field(..., description="ID landing page")
    title: str
    slug: str
    language: str
    meta: dict  # Flexibiln√≠ meta objekt
    contentHtml: str
    visuals: Optional[dict] = None  # Flexibiln√≠ visuals objekt
    createdAt: str
    updatedAt: str

@app.post("/api/batch-pipeline", response_model=BatchPipelineResponse)
async def batch_pipeline_run(request: BatchPipelineRequest):
    """
    üöÄ BATCH PROCESSING: Spust√≠ SEO pipeline pro v≈°echna t√©mata z CSV souƒçasnƒõ.
    
    Args:
        request: Batch request s project_id a CSV souborem
        
    Returns:
        Response s batch ID a seznamem spu≈°tƒõn√Ωch workflow
    """
    import base64
    import csv
    import io
    from datetime import datetime
    
    try:
        logger.info(f"üöÄ BATCH PROCESSING STARTED:")
        logger.info(f"   üèóÔ∏è Project ID: {request.project_id}")
        logger.info(f"   üìÑ Batch: {request.batch_name or 'Bez n√°zvu'}")
        
        # Ovƒõ≈ôen√≠ existence projektu
        prisma = await get_prisma_client()
        project = await prisma.project.find_unique(where={"id": request.project_id})
        if not project:
            logger.error(f"‚ùå Projekt s ID {request.project_id} nenalezen")
            raise HTTPException(status_code=400, detail=f"Projekt s ID {request.project_id} neexistuje")
        
        # Dek√≥dov√°n√≠ a parsov√°n√≠ CSV
        try:
            csv_content = base64.b64decode(request.csv.content).decode('utf-8')
            csv_reader = csv.reader(io.StringIO(csv_content))
            
            topics = []
            for row_idx, row in enumerate(csv_reader):
                if row_idx == 0:  # Skip header
                    continue
                if row and row[0].strip():  # Non-empty topic
                    topics.append(row[0].strip())
            
            logger.info(f"üìã Parsov√°no {len(topics)} t√©mat z CSV")
            
        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi parsov√°n√≠ CSV: {str(e)}")
            raise HTTPException(status_code=400, detail=f"Neplatn√Ω CSV form√°t: {str(e)}")
        
        if not topics:
            raise HTTPException(status_code=400, detail="CSV neobsahuje ≈æ√°dn√° t√©mata")
        
        # Batch ID pro tracking
        batch_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(topics)}"
        logger.info(f"üÜî Batch ID: {batch_id}")
        
        # Spu≈°tƒõn√≠ workflow pro ka≈æd√© t√©ma
        workflow_ids = []
        failed_topics = []
        
        for i, topic in enumerate(topics):
            try:
                logger.info(f"üöÄ Spou≈°t√≠m workflow {i+1}/{len(topics)}: '{topic}'")
                
                workflow_id, run_id = await start_seo_pipeline(
                    topic=topic,
                    project_id=request.project_id,
                    csv_base64=None  # Individual workflow, no CSV needed
                )
                
                workflow_ids.append(workflow_id)
                
                # Vytvo≈ôen√≠ datab√°zov√©ho z√°znamu
                from api.routes.workflow_run import WorkflowRunCreate, create_workflow_run
                
                workflow_run_data = WorkflowRunCreate(
                    projectId=request.project_id,
                    topic=f"[BATCH:{batch_id}] {topic}",
                    runId=run_id,
                    workflowId=workflow_id
                )
                
                await create_workflow_run(workflow_run_data)
                logger.info(f"‚úÖ Workflow {i+1} spu≈°tƒõn: {workflow_id}")
                
            except Exception as e:
                logger.error(f"‚ùå Chyba p≈ôi spou≈°tƒõn√≠ workflow pro '{topic}': {str(e)}")
                failed_topics.append(f"{topic}: {str(e)}")
        
        success_count = len(workflow_ids)
        logger.info(f"üéâ BATCH COMPLETED:")
        logger.info(f"   ‚úÖ √öspƒõ≈°nƒõ: {success_count}/{len(topics)}")
        logger.info(f"   ‚ùå Chyby: {len(failed_topics)}")
        
        if failed_topics:
            logger.warning(f"‚ùå Ne√∫spƒõ≈°n√° t√©mata: {failed_topics}")
        
        return BatchPipelineResponse(
            status=f"Batch spu≈°tƒõn: {success_count}/{len(topics)} workflow",
            batch_id=batch_id,
            total_workflows=success_count,
            workflow_ids=workflow_ids
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå BATCH PROCESSING FAILED: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Chyba p≈ôi batch processing: {str(e)}")

@app.post("/api/pipeline-run", response_model=PipelineResponse)
async def pipeline_run(request: PipelineRequest):
    """
    Spust√≠ SINGLE SEO pipeline workflow p≈ôes Temporal a vytvo≈ô√≠ z√°znam v datab√°zi.
    
    Args:
        request: Pipeline request s t√©matem, project_id a voliteln√Ωm CSV
        
    Returns:
        Response s workflow ID, run ID a datab√°zov√Ωm ID
        
    Raises:
        HTTPException: 400 pokud projekt neexistuje, 500 pokud chyb√≠ p≈ôipojen√≠ k Temporal
    """
    try:
        logger.info(f"üöÄ Spou≈°t√≠m SEO pipeline:")
        logger.info(f"   üìã T√©ma: {request.topic}")
        logger.info(f"   üèóÔ∏è Project ID: {request.project_id}")
        logger.info(f"   üìÑ CSV: {'‚úÖ P≈ôilo≈æen' if request.csv else '‚ùå ≈Ω√°dn√Ω'}")
        
        # Ovƒõ≈ôen√≠ existence projektu pokud je zad√°n project_id
        database_id = None
        if request.project_id:
            try:
                prisma = await get_prisma_client()
                project = await prisma.project.find_unique(where={"id": request.project_id})
                if not project:
                    logger.error(f"‚ùå Projekt s ID {request.project_id} nenalezen")
                    raise HTTPException(status_code=400, detail=f"Projekt s ID {request.project_id} neexistuje")
                logger.info(f"‚úÖ Projekt ovƒõ≈ôen: {project.name}")
            except HTTPException:
                raise
            except Exception as e:
                logger.error(f"‚ùå Chyba p≈ôi ovƒõ≈ôov√°n√≠ projektu: {str(e)}")
                raise HTTPException(status_code=500, detail=f"Chyba p≈ôi ovƒõ≈ôov√°n√≠ projektu: {str(e)}")
        
        # Extrakce CSV obsahu pokud existuje
        csv_base64 = None
        if request.csv:
            csv_base64 = request.csv.content
            logger.info(f"üìÑ CSV soubor p≈ôilo≈æen: {request.csv.name}")
        
        # Spu≈°tƒõn√≠ Temporal workflow
        logger.info("üîå P≈ôipojuji se k Temporal serveru...")
        workflow_id, run_id = await start_seo_pipeline(
            topic=request.topic,
            project_id=request.project_id,
            csv_base64=csv_base64
        )
        
        logger.info(f"‚úÖ Temporal workflow √∫spƒõ≈°nƒõ spu≈°tƒõn:")
        logger.info(f"   üÜî Workflow ID: {workflow_id}")
        logger.info(f"   üèÉ Run ID: {run_id}")
        
        # Vytvo≈ôen√≠ z√°znamu v datab√°zi pokud je zad√°n project_id
        if request.project_id:
            try:
                # Import WorkflowRunCreate modelu a create_workflow_run funkce
                from api.routes.workflow_run import WorkflowRunCreate, create_workflow_run
                
                # Vytvo≈ôen√≠ z√°znamu workflow run v datab√°zi
                workflow_run_data = WorkflowRunCreate(
                    projectId=request.project_id,
                    topic=request.topic,
                    runId=run_id,
                    workflowId=workflow_id
                )
                
                logger.info(f"üíæ Ukl√°d√°m workflow do datab√°ze:")
                logger.info(f"   üìù Topic: {request.topic}")
                logger.info(f"   üèóÔ∏è Project ID: {request.project_id}")
                logger.info(f"   üÜî Workflow ID: {workflow_id}")
                logger.info(f"   üèÉ Run ID: {run_id}")
                
                # Skuteƒçn√© vol√°n√≠ API endpointu pro vytvo≈ôen√≠ datab√°zov√©ho z√°znamu
                workflow_response = await create_workflow_run(workflow_run_data)
                database_id = workflow_response.id
                
                logger.info(f"‚úÖ Workflow run skuteƒçnƒõ vytvo≈ôen v datab√°zi s ID: {database_id}")
                
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Chyba p≈ôi vytv√°≈ôen√≠ datab√°zov√©ho z√°znamu: {str(e)}")
                logger.info("‚ÑπÔ∏è Workflow pokraƒçuje, ale bez datab√°zov√©ho z√°znamu")
        
        logger.info(f"üéâ Pipeline √∫spƒõ≈°nƒõ spu≈°tƒõna pro t√©ma: '{request.topic}'")
        
        return PipelineResponse(
            status="started",
            workflow_id=workflow_id,
            run_id=run_id,
            project_id=request.project_id,
            database_id=database_id
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Kritick√° chyba p≈ôi spu≈°tƒõn√≠ pipeline:")
        logger.error(f"   üìã T√©ma: {request.topic}")
        logger.error(f"   üèóÔ∏è Project ID: {request.project_id}")
        logger.error(f"   üö® Chyba: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Chyba p≈ôi spu≈°tƒõn√≠ workflow: {str(e)}"
        )

@app.get("/api/workflows")
async def get_workflows(limit: int = Query(30, description="Maxim√°ln√≠ poƒçet v√Ωsledk≈Ø")):
    """
    Naƒçte seznam workflow executions z Temporal serveru.
    
    Args:
        limit: Maxim√°ln√≠ poƒçet v√Ωsledk≈Ø (default 30)
        
    Returns:
        JSON s workflows seznamem
        
    Raises:
        HTTPException: Appropriate HTTP status based on error type
    """
    logger.info(f"üß† Dotaz na Temporal: naƒç√≠t√°m {limit} workflows...")
    
    try:
        # Vol√°n√≠ funkce pro naƒçten√≠ workflows
        workflows = await list_workflows(limit=limit)
        
        if not workflows:
            logger.info("üì≠ ≈Ω√°dn√© workflows nenalezeny - vr√°c√≠m pr√°zdn√© pole")
            return {"workflows": []}
        
        logger.info(f"‚úÖ Vr√°ceno {len(workflows)} workflow≈Ø")
        return {"workflows": workflows}
        
    except ConnectionError as e:
        logger.error(f"‚ùå Temporal server nedostupn√Ω: {str(e)}")
        raise HTTPException(
            status_code=503,
            detail="Temporal server je moment√°lnƒõ nedostupn√Ω. Zkuste to znovu pozdƒõji."
        )
    except ValueError as e:
        logger.error(f"‚ùå Neplatn√° data z Temporal: {str(e)}")
        raise HTTPException(
            status_code=422,
            detail=f"Chyba zpracov√°n√≠ dat: {str(e)}"
        )
    except Exception as e:
        # Loguj cel√Ω stacktrace pro debug
        import traceback
        logger.error(f"‚ùå Neoƒçek√°v√°n√° chyba p≈ôi naƒç√≠t√°n√≠ workflows:")
        logger.error(f"   Typ: {type(e).__name__}")
        logger.error(f"   Zpr√°va: {str(e)}")
        logger.error(f"   Stacktrace: {traceback.format_exc()}")
        
        raise HTTPException(
            status_code=500,
            detail=f"Chyba p≈ôi naƒç√≠t√°n√≠ workflows: {str(e)}"
        )

@app.get("/api/workflow-result/{workflow_id}/{run_id}")
async def get_workflow_result_endpoint(
    workflow_id: str = Path(..., description="ID workflow"),
    run_id: str = Path(..., description="Run ID workflow")
):
    """
    Z√≠sk√° v√Ωsledek dokonƒçen√©ho workflow z Temporal serveru s diagnostick√Ωmi informacemi.
    
    Args:
        workflow_id: ID workflow
        run_id: Run ID workflow
        
    Returns:
        JSON s workflow v√Ωsledkem, metadata a diagnostick√Ωmi informacemi
        
    Raises:
        HTTPException: 404 pokud workflow neexistuje, 503 pokud Temporal nen√≠ dostupn√Ω
    """
    logger.info(f"üì§ Fetch result: workflow_id={workflow_id}, run_id={run_id}")
    
    try:
        # üîß OPRAVA: Nejd≈ô√≠v zkus√≠me naƒç√≠st aktualizovan√° data z datab√°ze
        from api.database import get_prisma_client
        prisma = await get_prisma_client()
        
        # Hled√°me workflow v datab√°zi
        db_run = await prisma.workflowrun.find_unique(
            where={
                "workflowId_runId": {
                    "workflowId": workflow_id,
                    "runId": run_id
                }
            }
        )
        

        
        # Pokud m√°me ulo≈æen√° aktualizovan√° data z retry, pou≈æijeme je
        if db_run and db_run.resultJson:
            try:
                import json
                result_data = json.loads(db_run.resultJson)

                logger.info("‚úÖ Naƒçtena aktualizovan√° data z datab√°ze (vƒçetnƒõ retry zmƒõn)")
            except Exception as e:
                # Fallback na Temporal pokud JSON parsing sel≈æe
                result_data = await get_workflow_result(workflow_id=workflow_id, run_id=run_id)
                logger.warning(f"JSON parsing failed, using Temporal data: {e}")
        else:
            # Naƒçteme z Temporal a aktualizujeme datab√°zi
            logger.info("No database data found, loading from Temporal")
            result_data = await get_workflow_result(workflow_id=workflow_id, run_id=run_id)
            await update_workflow_status_in_database(workflow_id=workflow_id, run_id=run_id, result_data=result_data)
            logger.info("‚úÖ Naƒçtena fresh data z Temporal")
        
        # P≈ôid√°me diagnostick√© informace pro RUNNING i TIMED_OUT workflow  
        if result_data.get("status") in ["RUNNING", "TIMED_OUT", "FAILED"]:
            try:
                diagnostic_info = await describe_workflow_execution(workflow_id=workflow_id, run_id=run_id)
                
                # Slouƒç√≠me diagnostick√© informace s v√Ωsledkem
                result_data.update({
                    "current_phase": diagnostic_info.get("current_phase", "Unknown"),
                    "current_activity_type": diagnostic_info.get("current_activity_type"),
                    "elapsed_seconds": diagnostic_info.get("elapsed_seconds", 0),
                    "activity_elapsed_seconds": diagnostic_info.get("activity_elapsed_seconds", 0),
                    "activity_attempt": diagnostic_info.get("activity_attempt", 0),
                    "is_long_running": diagnostic_info.get("is_long_running", False),
                    "warning": diagnostic_info.get("warning", False),
                    "workflow_history": diagnostic_info.get("workflow_history", [])  # üîç AUDIT: Historie aktivit
                })
                
                logger.info(f"üéØ Current phase: {diagnostic_info.get('current_phase')} ({diagnostic_info.get('elapsed_seconds', 0)/60:.1f} min)")
                
            except Exception as diag_error:
                logger.warning(f"‚ö†Ô∏è Diagnostika selhala: {str(diag_error)}")
                # P≈ôid√°me alespo≈à basic info
                result_data.update({
                    "current_phase": "Unknown (diagnostic failed)",
                    "warning": False,
                    "diagnostic_error": str(diag_error)
                })
        
        logger.info(f"‚úÖ Result loaded: status={result_data.get('status')}")
        return result_data
        
    except ValueError as e:
        # Workflow neexistuje nebo nen√≠ dokonƒçen
        logger.warning(f"‚ö†Ô∏è Workflow nenalezen: {str(e)}")
        raise HTTPException(
            status_code=404,
            detail={
                "error": "Workflow nenalezen nebo v√Ωstup nen√≠ k dispozici",
                "message": str(e)
            }
        )
    except ConnectionError as e:
        # Temporal server nen√≠ dostupn√Ω
        logger.error(f"‚ùå Temporal connection error: {str(e)}")
        raise HTTPException(
            status_code=503,
            detail={
                "error": "Temporal server nen√≠ dostupn√Ω",
                "message": "Zkuste to pozdƒõji nebo kontaktujte administr√°tora"
            }
        )
    except Exception as e:
        # Ostatn√≠ chyby
        logger.error(f"‚ùå Error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "Nastala chyba p≈ôi naƒç√≠t√°n√≠ v√Ωstupu",
                "message": str(e)
            }
        )

@app.post("/api/workflow-terminate/{workflow_id}/{run_id}")
async def terminate_workflow_endpoint(
    workflow_id: str = Path(..., description="ID workflow"),
    run_id: str = Path(..., description="Run ID workflow"),
    request: TerminateWorkflowRequest = None
):
    """
    Ukonƒç√≠ bƒõ≈æ√≠c√≠ workflow execution.
    
    Args:
        workflow_id: ID workflow
        run_id: Run ID workflow  
        request: Po≈æadavek s d≈Øvodem ukonƒçen√≠
        
    Returns:
        JSON s potvrzen√≠m ukonƒçen√≠
        
    Raises:
        HTTPException: 404 pokud workflow neexistuje nebo nen√≠ RUNNING, 503 pokud Temporal nen√≠ dostupn√Ω
    """
    reason = request.reason if request else "Manually terminated by user"
    logger.info(f"‚õî Terminate request: workflow_id={workflow_id}, run_id={run_id}, reason={reason}")
    
    try:
        result = await terminate_workflow(workflow_id=workflow_id, run_id=run_id, reason=reason)
        logger.info(f"‚úÖ Workflow terminated successfully: {workflow_id}")
        return result
        
    except ValueError as e:
        # Workflow neexistuje nebo nen√≠ RUNNING
        logger.warning(f"‚ö†Ô∏è Cannot terminate workflow: {str(e)}")
        raise HTTPException(
            status_code=404,
            detail={
                "error": "Workflow nelze ukonƒçit",
                "message": str(e)
            }
        )
    except ConnectionError as e:
        # Temporal server nen√≠ dostupn√Ω
        logger.error(f"‚ùå Temporal connection error: {str(e)}")
        raise HTTPException(
            status_code=503,
            detail={
                "error": "Temporal server nen√≠ dostupn√Ω",
                "message": "Zkuste to pozdƒõji nebo kontaktujte administr√°tora"
            }
        )
    except Exception as e:
        # Ostatn√≠ chyby
        logger.error(f"‚ùå Error terminating workflow: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "Nastala chyba p≈ôi ukonƒçov√°n√≠ workflow",
                "message": str(e)
            }
        )

async def update_workflow_status_in_database(workflow_id: str, run_id: str, result_data: dict):
    """
    Aktualizuje status workflow v datab√°zi na z√°kladƒõ informac√≠ z Temporal serveru.
    
    Args:
        workflow_id: ID workflow z Temporal
        run_id: Run ID workflow z Temporal 
        result_data: V√Ωsledek z get_workflow_result
    """
    try:
        from api.routes.workflow_run import get_prisma_client
        from datetime import datetime
        

        
        prisma = await get_prisma_client()

        
        # Najdeme workflow run podle workflowId a runId
        existing_run = await prisma.workflowrun.find_unique(
            where={
                "workflowId_runId": {
                    "workflowId": workflow_id,
                    "runId": run_id
                }
            }
        )
        
        if not existing_run:
            logger.warning(f"‚ö†Ô∏è Workflow run {workflow_id}/{run_id} nenalezen v datab√°zi pro aktualizaci")
            return
        
        # P≈ôiprav√≠me data pro aktualizaci
        update_fields = {}
        temporal_status = result_data.get("status")
        
        # Mapov√°n√≠ Temporal status≈Ø na na≈°e datab√°zov√© statusy
        if temporal_status == "COMPLETED":
            update_fields["status"] = "COMPLETED"
            if result_data.get("end_time"):
                update_fields["finishedAt"] = datetime.fromisoformat(result_data["end_time"].replace('Z', '+00:00'))
        elif temporal_status == "FAILED":
            update_fields["status"] = "FAILED"
            if result_data.get("end_time"):
                update_fields["finishedAt"] = datetime.fromisoformat(result_data["end_time"].replace('Z', '+00:00'))
        elif temporal_status == "TIMED_OUT":
            update_fields["status"] = "TIMED_OUT"
            if result_data.get("end_time"):
                update_fields["finishedAt"] = datetime.fromisoformat(result_data["end_time"].replace('Z', '+00:00'))
        elif temporal_status == "RUNNING":
            update_fields["status"] = "RUNNING"
        else:
            update_fields["status"] = temporal_status or "UNKNOWN"
        
        # P≈ôid√°me v√Ωsledek jako JSON pokud existuje
        if result_data.get("result"):
            import json
            update_fields["resultJson"] = json.dumps(result_data["result"], ensure_ascii=False)
        
        # P≈ôid√°me stage informace pokud existuj√≠
        if result_data.get("stage_logs"):
            completed_stages = len([log for log in result_data["stage_logs"] if log.get("status") == "COMPLETED"])
            total_stages = len(result_data["stage_logs"])
            update_fields["stageCount"] = completed_stages
            update_fields["totalStages"] = total_stages
        
        # üîß KRITICK√â: Ulo≈æ√≠me celou aktualizovanou strukturu do datab√°ze
        # Frontend pot≈ôebuje p≈ô√≠stup k aktualizovan√Ωm stages po retry
        # Pou≈æ√≠v√°me existuj√≠c√≠ pole resultJson m√≠sto vytv√°≈ôen√≠ nov√©ho
        update_fields["resultJson"] = json.dumps(result_data, ensure_ascii=False)
        logger.info(f"üíæ Ukl√°d√°m celou workflow strukturu do datab√°ze (vƒçetnƒõ stages)")
        

        
        # Aktualizace v datab√°zi
        updated_run = await prisma.workflowrun.update(
            where={"id": existing_run.id},
            data=update_fields
        )
        
        logger.info(f"‚úÖ Database updated: {updated_run.status} ({updated_run.stageCount}/{updated_run.totalStages} stages)")
        
    except Exception as e:
        logger.error(f"‚ùå Database update failed: {str(e)}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        # Nebudeme hadit exception, aby to nerozhodilo hlavn√≠ flow


@app.post("/api/retry-publish-script")
async def retry_publish_script(request: dict):
    """
    Spust√≠ pouze PublishScript bez cel√© pipeline pro √∫sporu AI kredit≈Ø
    """
    try:
        workflow_id = request.get("workflow_id")
        run_id = request.get("run_id")
        stage = request.get("stage")
        
        if not workflow_id or not run_id:
            raise HTTPException(status_code=400, detail="workflow_id a run_id jsou povinn√©")
        
        logger.info(f"üîß Retry PublishScript:")
        logger.info(f"   üÜî URL Workflow ID: {workflow_id}")
        logger.info(f"   üèÉ URL Run ID: {run_id}")
        logger.info(f"   üìã Stage: {stage}")
        
        # üîç OPRAVA: Nejd≈ô√≠v resolvujeme spr√°vn√© Temporal IDs z datab√°ze
        from api.database import get_prisma_client
        
        try:
            prisma = await get_prisma_client()
            
            # Hled√°me workflow v datab√°zi podle URL parametr≈Ø
            # URL parametry mohou b√Ωt ƒç√°st n√°zvu workflow_id, tak≈æe hled√°me p≈ôes LIKE
            workflow_records = await prisma.workflowrun.find_many(
                where={
                    "OR": [
                        {"workflowId": {"contains": workflow_id}},
                        {"runId": run_id},
                        {"id": run_id}  # mo≈æn√° run_id je database ID
                    ]
                }
            )
            
            if not workflow_records:
                logger.error(f"‚ùå Nena≈°el jsem workflow v datab√°zi pro URL parametry")
                raise HTTPException(status_code=404, detail=f"Workflow nenalezen v datab√°zi pro zadan√© parametry")
            
            # Bereme prvn√≠ (a pravdƒõpodobnƒõ jedin√Ω) z√°znam
            workflow_record = workflow_records[0]
            actual_workflow_id = workflow_record.workflowId
            actual_run_id = workflow_record.runId
            
            logger.info(f"‚úÖ Resolvov√°ny spr√°vn√© Temporal IDs:")
            logger.info(f"   üéØ Skuteƒçn√Ω Workflow ID: {actual_workflow_id}")
            logger.info(f"   üéØ Skuteƒçn√Ω Run ID: {actual_run_id}")
            
        except Exception as db_error:
            logger.error(f"‚ùå Chyba p≈ôi hled√°n√≠ workflow v datab√°zi: {db_error}")
            raise HTTPException(status_code=500, detail=f"Chyba p≈ôi hled√°n√≠ workflow: {str(db_error)}")
        
        # Naƒçteme workflow v√Ωsledek pro z√≠sk√°n√≠ pipeline dat
        from temporal_client import get_workflow_result
        
        workflow_result = await get_workflow_result(actual_workflow_id, actual_run_id)
        
        if not workflow_result or not workflow_result.get("result"):
            raise HTTPException(status_code=404, detail="Workflow data nenalezena")
        
        pipeline_data = workflow_result["result"]
        
        # Spust√≠me pouze publish_activity s aktu√°ln√≠mi daty
        from temporalio.client import Client
        import sys
        import os
        sys.path.append(os.path.dirname(os.path.dirname(__file__)))
        from workflows.assistant_pipeline_workflow import AssistantPipelineWorkflow
        
        # P≈ôipojen√≠ k Temporal
        client = await Client.connect("localhost:7233")
        
        # DIAGNOSTIKA: Zkontrolujme strukturu dat
        logger.info(f"üîç Pipeline data keys: {list(pipeline_data.keys())}")
        logger.info(f"üîç Pipeline final_output sample: {str(pipeline_data.get('final_output', 'MISSING'))[:300]}...")
        
        # Pipeline data obsahuje stage_logs, ale pot≈ôebujeme fin√°ln√≠ v√Ωstup
        final_output = pipeline_data.get("final_output", "")
        
        if isinstance(final_output, str) and final_output.strip().startswith('{'):
            # Pokud je final_output JSON string, parsujeme ho
            try:
                import json
                final_output = json.loads(final_output)
                logger.info("‚úÖ Final output √∫spƒõ≈°nƒõ parsov√°n jako JSON")
            except:
                logger.warning("‚ö†Ô∏è Nelze parsovat final output jako JSON")
        
        # SPR√ÅVN√Å EXTRAKCE DAT ze stage_logs
        stage_logs = pipeline_data.get("stage_logs", [])
        logger.info(f"üìä Nalezeno {len(stage_logs)} stage logs")
        
        # CHRONOLOGICK√Å DIAGNOSTIKA PIPELINE
        logger.info("üîç CHRONOLOGIE PIPELINE:")
        for i, log in enumerate(stage_logs):
            stage = log.get("stage", "UNKNOWN")
            status = log.get("status", "UNKNOWN")
            output_preview = str(log.get("output", ""))[:100] + "..." if log.get("output") else "NO OUTPUT"
            logger.info(f"  {i+1:2d}. {stage} - {status} - {output_preview}")
        
        # Extrakce v√Ωstup≈Ø asistent≈Ø ze stage_logs
        components = {
            "draft_assistant_output": "",
            "seo_assistant_output": "",
            "humanizer_assistant_output": "",
            "humanizer_output_after_fact_validation": "",  # ‚úÖ P≈òID√ÅNO
            "multimedia_assistant_output": "",
            "image_renderer_assistant_output": "",
            "qa_assistant_output": "",
            "fact_validator_assistant_output": "",
            "brief_assistant_output": ""
        }
        
        # Projdeme stage_logs a najdeme v√Ωstupy jednotliv√Ωch asistent≈Ø
        for log in stage_logs:
            stage_name = log.get("stage", "")
            output = log.get("output", "")
            
            if stage_name and output:
                # üîç DETAILN√ç DEBUG MAPPING
                logger.info(f"üîç Zpracov√°v√°m stage: '{stage_name}' -> output length: {len(str(output))}")
                
                # Mapov√°n√≠ stage names na component keys
                if "seo" in stage_name.lower():
                    components["seo_assistant_output"] = output
                    logger.info(f"‚úÖ SEO output √öSPƒö≈†Nƒö namapov√°n: {len(str(output))} znak≈Ø")
                    logger.info(f"üîç SEO output preview: {str(output)[:200]}...")
                elif "draft" in stage_name.lower():
                    components["draft_assistant_output"] = output
                    logger.info(f"‚úÖ Draft output nalezen: {len(str(output))} znak≈Ø")
                elif "humanizer" in stage_name.lower():
                    components["humanizer_assistant_output"] = output
                    components["humanizer_output_after_fact_validation"] = output  # ‚úÖ P≈òID√ÅNO - same jako humanizer
                    logger.info(f"‚úÖ Humanizer output nalezen: {len(str(output))} znak≈Ø")
                    logger.info(f"‚úÖ Humanizer tak√© namapov√°n jako humanizer_output_after_fact_validation")
                elif "multimedia" in stage_name.lower():
                    components["multimedia_assistant_output"] = output
                    logger.info(f"‚úÖ Multimedia output nalezen: {len(str(output))} znak≈Ø")
                elif "image" in stage_name.lower():
                    components["image_renderer_assistant_output"] = output
                    logger.info(f"‚úÖ Image output nalezen: {len(str(output))} znak≈Ø")
                elif "qa" in stage_name.lower():
                    components["qa_assistant_output"] = output
                    logger.info(f"‚úÖ QA output nalezen: {len(str(output))} znak≈Ø")
                elif "fact" in stage_name.lower() or "validator" in stage_name.lower():
                    components["fact_validator_assistant_output"] = output
                    logger.info(f"‚úÖ FactValidator output nalezen: {len(str(output))} znak≈Ø")
                elif "brief" in stage_name.lower():
                    components["brief_assistant_output"] = output
                    logger.info(f"‚úÖ Brief output nalezen: {len(str(output))} znak≈Ø")
        
        logger.info(f"üìä Extrakce dokonƒçena: {sum(1 for v in components.values() if v)} nepr√°zdn√Ωch v√Ωstup≈Ø")
        
        # DIRECT SCRIPT CALL - spust√≠me publish_script p≈ô√≠mo bez Temporal
        try:
            from datetime import datetime
            import sys
            import os
            import json
            sys.path.append(os.path.dirname(os.path.dirname(__file__)))
            from helpers.transformers import transform_to_PublishInput
            from activities.publish_script import publish_script
            
            logger.info("üîß Spou≈°t√≠m PublishScript p≈ô√≠mo jako Python funkci...")
            logger.info(f"üìä Components keys: {list(components.keys())}")
            logger.info(f"üìä SEO output sample: {components.get('seo_assistant_output', 'MISSING')[:300]}...")
            logger.info(f"üìä Draft output sample: {components.get('draft_assistant_output', 'MISSING')[:200]}...")
            
            # Debug SEO parsov√°n√≠
            try:
                from helpers.transformers import parse_seo_metadata, parse_qa_faq
                seo_data = parse_seo_metadata(components.get('seo_assistant_output', ''))
                logger.info(f"üîç SEO parsov√°no: title={seo_data.get('title', 'MISSING')}")
                logger.info(f"üîç SEO keywords: {seo_data.get('keywords', [])} (count: {len(seo_data.get('keywords', []))})")
                
                # Debug QA parsov√°n√≠
                qa_data = parse_qa_faq(components.get('qa_assistant_output', ''))
                logger.info(f"üîç QA parsov√°no: {len(qa_data)} FAQ polo≈æek")
                logger.info(f"üîç QA sample: {components.get('qa_assistant_output', 'MISSING')[:500]}...")
            except Exception as e:
                logger.error(f"‚ùå Chyba p≈ôi parsov√°n√≠: {e}")
            
            # Transform pipeline data na PublishInput format - P≈òED transformac√≠ p≈ôeveƒè dict na string
            logger.info("üîß P≈ôev√°d√≠m dict objekty na stringy p≈ôed transformac√≠...")
            for key, value in components.items():
                if isinstance(value, dict):
                    components[key] = json.dumps(value, ensure_ascii=False)
                    logger.info(f"‚úÖ {key}: dict p≈ôeveden na JSON string")
                elif not isinstance(value, str):
                    components[key] = str(value)
                    logger.info(f"‚úÖ {key}: {type(value)} p≈ôeveden na string")
            
            # P≈ôid√°me current_date pro spr√°vn√Ω ISO 8601 form√°t
            components["current_date"] = components.get("current_date") or __import__('datetime').datetime.now(__import__('datetime').timezone.utc).isoformat().replace('+00:00', 'Z')
            logger.info(f"‚úÖ current_date nastaven: {components['current_date']}")
            
            publish_input = transform_to_PublishInput(components)
            logger.info(f"üìä Transformace dokonƒçena: {len(publish_input)} polo≈æek")
            
            # P≈ò√çMO SPUST√çME PUBLISH_SCRIPT
            result = publish_script(publish_input)
            
            # DEBUG: Co publish script vrac√≠?
            logger.info(f"üîç DEBUG: publish_script vr√°til typ: {type(result)}")
            logger.info(f"üîç DEBUG: publish_script keys: {list(result.keys()) if isinstance(result, dict) else 'NOT_DICT'}")
            logger.info(f"üîç DEBUG: success kl√≠ƒç: {result.get('success', 'MISSING_KEY') if isinstance(result, dict) else 'NOT_DICT'}")
            logger.info(f"üîç DEBUG: cel√Ω result (prvn√≠ 500 znak≈Ø): {str(result)[:500]}")
            
            retry_id = f"retry_publish_direct_{workflow_id}_{int(__import__('datetime').datetime.now().timestamp())}"
            logger.info(f"‚úÖ PublishScript retry dokonƒçen: {retry_id}")
            logger.info(f"üìä V√Ωsledek: {result.get('success', False)}")
            
            # üîß KRITICK√Å OPRAVA: Aktualizuj datab√°zi s nov√Ωmi regenerovan√Ωmi daty
            logger.info(f"üîç DEBUG: result.get('success') = {result.get('success')}")
            logger.info(f"üîç DEBUG: actual_workflow_id = {actual_workflow_id}")
            logger.info(f"üîç DEBUG: actual_run_id = {actual_run_id}")
            
            if result.get('success'):
                try:
                    logger.info("üîÑ Aktualizuji datab√°zi s nov√Ωmi regenerovan√Ωmi daty...")
                    
                    # Naƒçti aktu√°ln√≠ workflow data
                    from temporal_client import get_workflow_result
                    logger.info("üîç Vol√°m get_workflow_result...")
                    current_workflow_data = await get_workflow_result(actual_workflow_id, actual_run_id)
                    logger.info(f"üîç Workflow data loaded: {bool(current_workflow_data)}")
                    
                    # Debug workflow data structure
                    logger.info(f"üîç Workflow data type: {type(current_workflow_data)}")
                    logger.info(f"üîç Workflow data keys: {list(current_workflow_data.keys()) if isinstance(current_workflow_data, dict) else 'NOT_DICT'}")
                    logger.info(f"üîç Has 'stages' key: {'stages' in current_workflow_data if isinstance(current_workflow_data, dict) else False}")
                    
                    # üîß OPRAVA: Stages jsou v 'stage_logs', ne 'stages'
                    stages_key = "stage_logs" if "stage_logs" in current_workflow_data else "stages" 
                    
                    # Najdi PublishScript stage a updatuj jeho output
                    if current_workflow_data and stages_key in current_workflow_data:
                        stages = current_workflow_data[stages_key]
                        logger.info(f"üîç Poƒçet {stages_key}: {len(stages)}")
                        
                        # Pot≈ôebujeme p≈ôev√©st stage_logs na strukturu kompatibiln√≠ s frontend
                        # Frontend oƒçek√°v√° 'stages' s 'stage_name' a 'stage_output'
                        updated_stages = []
                        
                        for i, stage_log in enumerate(stages):
                            stage_name = stage_log.get("stage", "")
                            logger.info(f"üîç Stage {i}: {stage_name}")
                            
                            # P≈ôevod stage_log na frontend form√°t
                            stage_frontend = {
                                "stage": stage_name,  # ‚úÖ OPRAVENO: "stage" m√≠sto "stage_name"
                                "stage_output": stage_log.get("output", ""),
                                "status": stage_log.get("status", ""),
                                "start_time": stage_log.get("start_time", ""),
                                "end_time": stage_log.get("end_time", "")
                            }
                            
                            if "publish" in stage_name.lower():
                                # Aktualizuj PublishScript output s nov√Ωmi daty
                                logger.info(f"üéØ Na≈°el jsem PublishScript stage: {stage_name}")
                                stage_frontend["stage_output"] = result
                                stage_frontend["output"] = result  # üîß OPRAVA: P≈ôidat tak√© 'output' field pro frontend tlaƒç√≠tka
                                # ‚úÖ KRITICK√Å OPRAVA: Zmƒõn√≠ status na COMPLETED
                                if result.get('success'):
                                    stage_frontend["status"] = "COMPLETED"
                                    logger.info("‚úÖ PublishScript stage status zmƒõnƒõn na COMPLETED")
                                else:
                                    stage_frontend["status"] = "FAILED"
                                    logger.info("‚ùå PublishScript stage status z≈Øst√°v√° FAILED")
                                logger.info("‚úÖ PublishScript stage output aktualizov√°n")
                            
                            updated_stages.append(stage_frontend)
                        
                        # Aktualizuj strukturu pro frontend kompatibilitu
                        current_workflow_data["stages"] = updated_stages
                        logger.info(f"‚úÖ P≈ôevedeno {len(updated_stages)} stages do frontend form√°tu")
                        
                        # üîß KRITICK√Å OPRAVA: Synchronizuj stage_logs se stages pro frontend kompatibilitu
                        # Frontend ƒçte z stage_logs, ale retry aktualizuje stages
                        if "stage_logs" in current_workflow_data:
                            current_workflow_data["stage_logs"] = updated_stages.copy()
                            logger.info(f"‚úÖ Synchronizov√°ny stage_logs se stages pro frontend")
                        
                        # Ulo≈æ zpƒõt do datab√°ze
                        await update_workflow_status_in_database(actual_workflow_id, actual_run_id, current_workflow_data)
                    else:
                        if not current_workflow_data:
                            logger.warning("‚ö†Ô∏è current_workflow_data je falsy")
                        elif "stages" not in current_workflow_data:
                            logger.warning("‚ö†Ô∏è 'stages' kl√≠ƒç nen√≠ v current_workflow_data")
                            logger.warning(f"‚ö†Ô∏è Dostupn√© kl√≠ƒçe: {list(current_workflow_data.keys())}")
                        else:
                            logger.warning("‚ö†Ô∏è Nezn√°m√Ω probl√©m s workflow data")
                        
                except Exception as e:
                    logger.error(f"‚ùå Chyba p≈ôi aktualizaci datab√°ze: {e}")
                    import traceback
                    logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
                    # Pokraƒçujeme i p≈ôi chybƒõ - hlavn√≠ vƒõc je ≈æe retry probƒõhl
            else:
                logger.warning(f"‚ö†Ô∏è Retry result success={result.get('success')} - p≈ôeskakuji update datab√°ze")
            
            return {
                "status": "completed" if result.get('success') else "failed",
                "retry_id": retry_id,
                "original_workflow_id": workflow_id,
                "result": result,
                "message": "PublishScript byl dokonƒçen" if result.get('success') else "PublishScript selhal"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Chyba p≈ôi spu≈°tƒõn√≠ PublishScript retry: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Chyba p≈ôi spu≈°tƒõn√≠ retry: {str(e)}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Chyba v retry_publish_script: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Neoƒçek√°van√° chyba: {str(e)}")

# ===== üìä LANDING PAGES API ENDPOINT =====

@app.post("/api/landing-pages", response_model=LandingPageResponse)
async def create_landing_page(request: LandingPageRequest):
    """
    üìä VYTVO≈òEN√ç LANDING PAGE S STRUKTUROVAN√ùMI TABULKAMI
    
    Endpoint pro ukl√°d√°n√≠ landing pages s comparison/pricing/feature tabulkami
    optimalizovan√Ωmi pro GEO/LLM modely a SEO.
    
    Args:
        request: Landing page data s tabulkami
        
    Returns:
        Vytvo≈ôen√° landing page s ID a timestamps
    """
    from datetime import datetime
    import uuid
    import json
    
    try:
        logger.info(f"üìä VYTV√Å≈òEN√ç LANDING PAGE (NOV√Å STRUKTURA):")
        logger.info(f"   üìã Title: {request.title}")
        logger.info(f"   üîó Slug: {request.slug}")
        logger.info(f"   üåç Language: {request.language}")
        logger.info(f"   üìÑ Meta description: {request.meta.description[:50] if request.meta else 'N/A'}...")
        
        # SAFE logging pro visuals
        if request.visuals:
            logger.info(f"   üìä Comparison tables: {len(request.visuals.comparisonTables or [])}")
            logger.info(f"   üí∞ Pricing tables: {len(request.visuals.pricingTables or [])}")
            logger.info(f"   ‚öôÔ∏è Feature tables: {len(request.visuals.featureTables or [])}")
        else:
            logger.info(f"   üìä ≈Ω√°dn√© tabulky v request")
        
        # Generov√°n√≠ ID a timestamp
        landing_page_id = str(uuid.uuid4())
        now = datetime.now().isoformat()
        
        # SAFE: P≈ôevod struktur na JSON s fallbacky
        meta_dict = {}
        visuals_dict = {}
        
        try:
            # Meta informace (v≈ædy vytvo≈ôit, i pr√°zdn√©)
            meta_dict = {
                "description": request.meta.description if request.meta else "Popis ƒçl√°nku",
                "keywords": request.meta.keywords if request.meta else [],
                "ogImage": request.meta.ogImage if request.meta else ""
            }
            
            # Visuals (pouze pokud existuj√≠)
            if request.visuals:
                if request.visuals.comparisonTables:
                    visuals_dict["comparisonTables"] = [table.dict() for table in request.visuals.comparisonTables]
                if request.visuals.pricingTables:
                    visuals_dict["pricingTables"] = [table.dict() for table in request.visuals.pricingTables]
                if request.visuals.featureTables:
                    visuals_dict["featureTables"] = [table.dict() for table in request.visuals.featureTables]
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Chyba p≈ôi zpracov√°n√≠ meta/visuals (pou≈æ√≠v√°m fallbacky): {e}")
            # SAFE fallbacks
            meta_dict = {"description": "Popis ƒçl√°nku", "keywords": [], "ogImage": ""}
            visuals_dict = {}
        
        # Vytvo≈ôen√≠ response (SAFE)
        response = LandingPageResponse(
            id=landing_page_id,
            title=request.title or "ƒål√°nek",
            slug=request.slug or "clanek",
            language=request.language or "cs",
            meta=meta_dict,
            contentHtml=request.contentHtml or "<p>Obsah ƒçl√°nku</p>",
            visuals=visuals_dict if visuals_dict else None,
            createdAt=now,
            updatedAt=now
        )
        
        # Log pro debugging
        logger.info(f"‚úÖ Landing page vytvo≈ôena:")
        logger.info(f"   üÜî ID: {landing_page_id}")
        logger.info(f"   üìä Visuals obsahuj√≠: {len(visuals_dict)} typ≈Ø tabulek")
        
        # DEBUG: V√Ωpis prvn√≠ comparison table pokud existuje
        if visuals_dict.get("comparisonTables"):
            first_table = visuals_dict["comparisonTables"][0]
            logger.info(f"   üîç Prvn√≠ comparison table: {first_table['title']}")
            logger.info(f"   üìã Headers: {first_table['headers']}")
            logger.info(f"   üìä Rows: {len(first_table['rows'])}")
        
        return response
        
    except Exception as e:
        logger.error(f"‚ùå Chyba p≈ôi vytv√°≈ôen√≠ landing page: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Chyba p≈ôi vytv√°≈ôen√≠ landing page: {str(e)}")


@app.get("/api/landing-pages/{page_id}", response_model=LandingPageResponse)
async def get_landing_page(page_id: str):
    """
    üìñ NAƒåTEN√ç LANDING PAGE
    
    Args:
        page_id: ID landing page
        
    Returns:
        Landing page s tabulkami
    """
    try:
        logger.info(f"üìñ Naƒç√≠t√°m landing page: {page_id}")
        
        # TODO: Implementovat skuteƒçn√© naƒçten√≠ z datab√°ze
        # Pro teƒè vr√°t√≠me demo data
        
        from datetime import datetime
        
        demo_response = LandingPageResponse(
            id=page_id,
            title="Demo Landing Page",
            slug="demo-landing-page", 
            language="cs",
            meta={
                "description": "Demo landing page pro testov√°n√≠ API",
                "keywords": ["demo", "landing", "page"],
                "ogImage": ""
            },
            contentHtml="<h1>Demo obsah</h1><p>Toto je demo landing page.</p>",
            visuals=None,  # ≈Ω√°dn√© demo tabulky
            createdAt=datetime.now().isoformat(),
            updatedAt=datetime.now().isoformat()
        )
        
        logger.info(f"‚úÖ Landing page naƒçtena: {page_id}")
        return demo_response
        
    except Exception as e:
        logger.error(f"‚ùå Chyba p≈ôi naƒç√≠t√°n√≠ landing page: {str(e)}")
        raise HTTPException(status_code=404, detail=f"Landing page nenalezena: {page_id}")


@app.get("/health")
async def health_check():
    """Health check endpoint pro monitoring"""
    return {"status": "healthy", "service": "seo-farm-backend"} 